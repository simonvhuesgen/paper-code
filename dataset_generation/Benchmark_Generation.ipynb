{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7245c45e-f90e-4afb-a3b0-7484843b6b1a",
   "metadata": {},
   "source": [
    "### Use GPT-4o to take text chunks & image summaries (or image itself) as input and generate queries and reference answers that fit to the article and/or figure info and instruction (what kind of queries are acceptable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34560a7-2bc9-4f24-82dd-4c9769ea3f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "AZURE_OPENAI_API_KEY=\"\"\n",
    "AZURE_OPENAI_ENDPOINT=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63293423-3343-47f0-b868-9cc2ff2887d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "MODEL = 'gpt4o-240513'\n",
    "API_VERSION = \"2024-02-15-preview\"\n",
    "client = AzureOpenAI(\n",
    "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
    "    api_key=AZURE_OPENAI_API_KEY,\n",
    "    api_version=API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d4537e-4d61-41de-9f6d-ea24c65323a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the full texts of your data\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "s3_path = \"\"\n",
    "files = fs.glob(f\"{s3_path}*.parquet\")\n",
    "print(f\"Found {len(files)} files in the directory.\")\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    try:\n",
    "        df = pd.read_parquet(f\"s3://{file}\", filesystem=fs)\n",
    "        dfs.append(df)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping file {file} due to error: {e}\")\n",
    "\n",
    "df_full_texts = pd.concat(dfs, ignore_index=True)\n",
    "print(df_full_texts.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94702737-2b9a-454d-b69a-fe5476a82a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the generated captions of your figures\n",
    "image_summary_df = pd.read_csv(\"data/captions.csv\")\n",
    "all_chunks_df = pd.read_csv(\"data/399k_all_chunks.csv\")\n",
    "image_summary_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd54e366-a759-4e2f-8afc-7ede06ec1792",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = pd.read_csv(\"data/50k_captions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae8f4c8-6e5d-40ac-bee1-fa60fef927fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### instruction prompts for different eval categories\n",
    "## text-based\n",
    "numerical_prompt = \"\"\"Generate a query asking for numerical metrics from research papers. Follow these steps:\n",
    "Extract the domain (e.g., machine learning, blockchain) from the text.\n",
    "Frame the query to request exact values: 'What [metric] is reported for [concept]?'\n",
    "The answer must cite values from methodology/results sections. If values are implied (e.g., 'significant improvement'), add a disclaimer like 'typically reported as...'.\n",
    "Strict Rules:\n",
    "Never invent unsupported numbers.\n",
    "Reject queries if the context lacks numerical data and that do not relate to the domain of Computer Science.\n",
    "Examples:\n",
    "Context: 'PoS consensus reduces energy consumption by 99.99% compared to PoW (Buterin, 2022).'\n",
    "Query: 'What energy savings are reported for Proof-of-Stake vs. Proof-of-Work?'\n",
    "Answer: 'PoS reduces energy use by 99.99% compared to PoW.'\n",
    "text_sentence: 'PoS consensus reduces energy consumption by 99.99% compared to PoW'\n",
    "image_sentence: 'N/A'\n",
    "Context: 'Sharding improves database throughput by 10x (Chen et al., 2023).'\n",
    "Query: 'What throughput increase does sharding provide in distributed databases?'\n",
    "Answer: 'Sharding increases throughput by 10× in horizontally scaled systems.'\n",
    "text_sentence: 'Sharding improves database throughput by 10x'\n",
    "image_sentence: 'N/A'\"\"\"\n",
    "\n",
    "factual_prompt = \"\"\"Generate a query asking for a formal definition relating to the domain of Computer Science. Follow these steps:\n",
    "Identify key terms in the text.\n",
    "Frame the query along the lines of: 'How is [concept] defined?'\n",
    "The answer MUST paraphrase definitions verbatim from literature reviews/introductions.\n",
    "If definitions are incomplete, infer using related terms but add: 'Commonly understood as...'.\n",
    "Strict Rules:\n",
    "Never conflate concepts.\n",
    "Flag uncertainty with disclaimers.\n",
    "Reject queries that do not relate to the domain of Computer Science.\n",
    "Example:\n",
    "Context: 'Federated learning trains models on decentralized devices (Kairouz et al., 2021).'\n",
    "Query: 'How is federated learning defined?'\n",
    "Answer: 'Decentralized training where raw data remains on client devices.'\n",
    "text_sentece: 'Federated learning trains models on decentralized devices'\n",
    "image_sentence: 'N/A'\"\"\"\n",
    "\n",
    "compare_prompt = \"\"\"Generate a query comparing two concepts relating to the domain of Computer Science. Follow these steps:\n",
    "Identify contrasting terms in the text.\n",
    "Frame the query as: 'How do [Concept A] and [Concept B] differ in [aspect]?'\n",
    "The answer MUST cite explicit differences from discussion sections.\n",
    "If differences are indirect, infer but note: 'Based on general trends...'.\n",
    "Strict Rules:\n",
    "Never assume unstated trade-offs.\n",
    "Never conflate concepts.\n",
    "Flag uncertainty with disclaimers.\n",
    "Example:\n",
    "Context: 'RSA is slower but more compatible; ECC is faster but less adopted (Chen, 2021).'\n",
    "Query: 'Compare RSA and ECC encryption in speed and compatibility.'\n",
    "Answer: 'RSA is slower but widely compatible; ECC is faster but less adopted (Chen, 2021).'\n",
    "text_sentece: 'RSA is slower but more compatible; ECC is faster but less adopted'\n",
    "image_sentence: 'N/A'\"\"\"\n",
    "\n",
    "\n",
    "compound_multipart_prompt = '''Generate a multi-part query asking for components/stages and their evaluation. Follow these steps:\n",
    "1. Identify a concept with distinct components/stages from the text.\n",
    "2. Frame the query as: \"What are the [number] [components/stages] of [concept], and how are they measured/evaluated?\"\n",
    "3. The answer MUST cite methodology sections for components and evaluation sections for metrics.\n",
    "4. If context lacks evaluation criteria, state: \"Evaluation methods not described.\"\n",
    "\n",
    "Strict Rules:\n",
    "- Never invent components or metrics.\n",
    "- Reject queries if components/stages are unspecified or unrelated to Computer Science.\n",
    "\n",
    "Examples:\n",
    "Context: \"Blockchain consensus involves proposal, validation, and commit phases measured by latency (ms).\"\n",
    "Query: \"What are the three phases of blockchain consensus, and how is their latency measured?\"\n",
    "Answer: \"Proposal, validation, commit; measured in milliseconds (ms).\" \n",
    "text_sentence: \"Blockchain consensus involves proposal, validation, and commit phases measured by latency (ms).\"\n",
    "image_sentence: \"N/A\"\n",
    "Context: \"GAN training includes generator/discriminator steps evaluated with FID scores.\"\n",
    "Query: \"What are the two stages of GAN training, and what metric evaluates their performance?\"\n",
    "Answer: \"Generator and discriminator stages; evaluated using FID scores.\"\n",
    "text_sentence: \"GAN training includes generator/discriminator steps evaluated with FID scores.\"\n",
    "image_sentence: \"N/A\"'''\n",
    "\n",
    "trend_analysis_prompt = '''Generate a query about research trends or variable relationships. Follow these steps:\n",
    "1. Identify two variables/metrics (e.g., model size vs. accuracy) from the text.\n",
    "2. Frame the query as: \"What [trend/correlation] exists between [Variable A] and [Variable B] in [domain]?\"\n",
    "3. The answer MUST synthesize findings from results sections or survey papers.\n",
    "4. If trends are implied, add: \"Literature suggests...\"\n",
    "\n",
    "Strict Rules:\n",
    "- Never assert causation without explicit evidence.\n",
    "- Reject queries lacking variable comparisons or unrelated to Computer Science.\n",
    "\n",
    "Examples:\n",
    "Context: \"Model compression reduces accuracy by 2-5% but cuts inference time by 60%.\"\n",
    "Query: \"What trade-off exists between model compression and accuracy in NLP?\"\n",
    "Answer: \"Compression reduces accuracy by 2-5% while cutting inference time by 60%.\"\n",
    "text_sentence: \"Model compression reduces accuracy by 2-5% but cuts inference time by 60%.\"\n",
    "image_sentence: \"N/A\"\n",
    "Context: \"Energy consumption scales quadratically with transformer depth.\"\n",
    "Query: \"How does transformer depth correlate with energy usage?\"\n",
    "Answer: \"Quadratic scaling: doubling layers increases energy use 4×.\"\n",
    "text_sentence: \"Energy consumption scales quadratically with transformer depth.\"\n",
    "image_sentence: \"N/A\"'''\n",
    "\n",
    "limitation_identification_prompt = '''Generate a query about reported limitations. Follow these steps:\n",
    "1. Identify a concept/technique and its constraints from discussion/limitations sections.\n",
    "2. Frame the query as: \"What limitations exist for [concept] in [domain/use case]?\"\n",
    "3. The answer MUST paraphrase limitations verbatim. If indirect, state: \"Challenges include...\"\n",
    "\n",
    "Strict Rules:\n",
    "- Never conflate limitations across domains.\n",
    "- Reject queries without explicit limitation mentions or unrelated to Computer Science.\n",
    "\n",
    "Examples:\n",
    "Context: \"Differential privacy reduces utility in high-dimensional datasets due to noise.\"\n",
    "Query: \"What limitations does differential privacy introduce for high-dimensional data?\"\n",
    "Answer: \"Noise addition reduces data utility in high-dimensional spaces.\"\n",
    "text_sentence: \"Differential privacy reduces utility in high-dimensional datasets due to noise.\"\n",
    "image_sentence: \"N/A\"\n",
    "Context: \"Blockchain sharding risks cross-shard communication overhead.\"\n",
    "Query: \"What scalability challenges exist for blockchain sharding?\"\n",
    "Answer: \"Cross-shard communication creates significant overhead.\"\n",
    "text_sentence: \"Blockchain sharding risks cross-shard communication overhead.\"\n",
    "image_sentence: \"N/A\"'''\n",
    "\n",
    "## image-summary based\n",
    "feature_enumeration_prompt = \"\"\"Generate a query asking for design features relating to the domain of Computer Science. Follow these steps:\n",
    "Identify the system from the image summaries.\n",
    "Frame the query as: 'What features characterize [system] architectures?'\n",
    "The answer MUST list attributes (e.g., 'API Gateway') from the summary.\n",
    "If features are missing, reply: 'Key features not described'.\n",
    "Strict Rules:\n",
    "Only list explicitly described features.\n",
    "Reject queries that do not relate to the domain of Computer Science.\n",
    "Either make queries general enough for multiple papers to possibly answer it, or make sure its clear which system/ model/ feature/ phenomenon is asked for! \n",
    "Example:\n",
    "Context: 'Microservice diagram shows API Gateway and User Service.'\n",
    "Query: 'What defines a microservice architecture?'\n",
    "Answer: 'API Gateway and User Service.'\n",
    "text_sentence: 'N/A'\n",
    "image_sentence: 'Microservice diagram shows API Gateway and User Service.'\"\"\"\n",
    "\n",
    "visual_identification_prompt = \"\"\"Generate a query asking about components in technical diagrams. Follow these steps:\n",
    "Identify the system/architecture (e.g., transformers, zero-trust networks) from the image summaries relating to the domain of Computer Science.\n",
    "Frame the query along the lines of: 'What [components/features] are shown in [system] diagrams?'\n",
    "The answer must list elements explicitly described (e.g., 'encoder layers', 'API Gateway').\n",
    "If components are unclear, infer common ones but clarify: 'typically include...'.\n",
    "Strict Rules:\n",
    "Reject if the summary lacks component descriptions or queries that do not relate to the domain of Computer Science.\n",
    "Never assume unlabeled elements.\n",
    "Either make queries general enough for multiple papers to possibly answer it, or make sure its clear which system/ model/ feature/ phenomenon is asked for! \n",
    "Examples:\n",
    "Context: 'Transformer diagram shows stacked encoder/decoder layers with multi-head attention blocks.'\n",
    "Query: 'What layers define a transformer architecture?'\n",
    "Answer: 'Encoder/decoder stacks with multi-head attention layers.'\n",
    "text_sentence: 'N/A'\n",
    "image_sentence: 'Transformer diagram shows stacked encoder/decoder layers with multi-head attention blocks.'\n",
    "Context: 'Zero-trust network diagram includes microsegmentation and encrypted tunnels.'\n",
    "Query: 'What elements define a zero-trust network architecture?'\n",
    "Answer: 'Microsegmentation and encrypted communication channels.'\n",
    "text_sentence: 'N/A'\n",
    "image_sentence: 'Zero-trust network diagram includes microsegmentation and encrypted tunnels.'\"\"\"\n",
    "\n",
    "data_interpretation_prompt = \"\"\"Generate a query about trends in visualized data over extended periods (e.g., years, iterations). Follow these steps:\n",
    "Identify the metric (e.g., accuracy, latency, energy efficiency) and time axis (e.g., epochs, years) from the image summary.\n",
    "Frame the query to ask about long-term behavior relating to the domain of Computer Science:\n",
    "'What trend does the [metric] plot reveal over [time period]?'\n",
    "'How does [metric] evolve over time in [system]?'\n",
    "'What does the long-term [metric] curve suggest about [system] behavior?'\n",
    "The answer must synthesize multi-phase patterns (e.g., 'plateauing', 'exponential decay', 'linear growth') from the summary.\n",
    "If trends are unclear or short-term, state: 'No long-term trend described.'\n",
    "Strict Rules:\n",
    "Never guess. Reject summaries lacking time-axis descriptions.\n",
    "Use precise terms like 'asymptotic convergence' or 'periodic fluctuations'.\n",
    "Either make queries general enough for multiple papers to possibly answer it, or make sure its clear which system/ model/ feature/ phenomenon is asked for! \n",
    "Examples:\n",
    "Context:\n",
    "Image Summary: \"Energy efficiency graph declines by 15% over 5 hardware generations due to thermal throttling.\"\n",
    "Query: \"How does energy efficiency evolve across hardware iterations?\"\n",
    "Answer: \"Efficiency declines by 15% over 5 generations, likely due to thermal limitations.\n",
    "text_sentence: 'N/A'\n",
    "image_sentence: 'Energy efficiency graph declines by 15% over 5 hardware generations due to thermal throttling.'\"\"\"\n",
    "\n",
    "\n",
    "image_retrieval_prompt = '''Generate a query requesting a specific visualization. Follow these steps:\n",
    "1. Identify a phenomenon (e.g., latency distribution) from image summaries.\n",
    "2. Frame the query as: \"[chart/diagram] illustrating [phenomenon]\" OR \"Show the [trend] of [phenomenon/ concept/ metric] recently.\"\n",
    "3. The answer MUST reference explicit summaries (e.g., \"histogram,\" \"architecture diagram\").\n",
    "\n",
    "Strict Rules:\n",
    "- Never invent visualization types.\n",
    "- Reject queries if no image context exists or unrelated to Computer Science.\n",
    "- Either make queries general enough for multiple papers to possibly answer it, or make sure its clear which system/ model/ feature/ phenomenon is asked for! \n",
    "\n",
    "Examples:\n",
    "Context: \"Figure 3: Training loss curve with epoch vs. accuracy.\"\n",
    "Query: \"What chart type shows the relationship between training epochs and accuracy?\"\n",
    "Answer: \"Line chart plotting accuracy against epochs.\"\n",
    "text_sentence: \"N/A\"\n",
    "image_sentence: \"Figure 3: Training loss curve with epoch vs. accuracy.\"\n",
    "Context: \"System diagram includes load balancers and worker nodes.\"\n",
    "Query: \"What diagram type represents the distributed system architecture?\"\n",
    "Answer: \"Component diagram with load balancers and worker nodes.\"\n",
    "text_sentence: \"N/A\"\n",
    "image_sentence: \"System diagram includes load balancers and worker nodes.\"'''\n",
    "\n",
    "functional_flow_prompt = '''Generate a query about process steps in diagrams. Follow these steps:\n",
    "1. Identify a system/process (e.g., API request handling) from image summaries.\n",
    "2. Frame the query as: \"What sequence is shown in [system] diagrams?\" \n",
    "3. The answer MUST list steps (e.g., \"1. Request, 2. Authentication, 3. Response\") from flowcharts.\n",
    "\n",
    "Strict Rules:\n",
    "- Never infer unlabeled steps.\n",
    "- Reject queries lacking flowchart/sequence context or unrelated to Computer Science.\n",
    "- Either make queries general enough for multiple papers to possibly answer it, or make sure its clear which system/ model/ feature/ phenomenon is asked for! \n",
    "\n",
    "Examples:\n",
    "Context: \"Flowchart: User login → Token generation → Access grant.\"\n",
    "Query: \"What steps are shown in the authentication flowchart?\"\n",
    "Answer: \"1. User login, 2. Token generation, 3. Access grant.\"\n",
    "text_sentence: \"N/A\"\n",
    "image_sentence: \"Flowchart: User login → Token generation → Access grant.\"\n",
    "Context: \"Diagram illustrates data ingestion → preprocessing → model training.\"\n",
    "Query: \"What sequence does the ML pipeline diagram depict?\"\n",
    "Answer: \"Data ingestion, preprocessing, then model training.\"\n",
    "text_sentence: \"N/A\"\n",
    "image_sentence: \"Diagram illustrates data ingestion → preprocessing → model training.\"'''\n",
    "\n",
    "annotation_prompt = '''Generate a query about diagram annotations. Follow these steps:\n",
    "1. Identify labels/symbols (e.g., arrows, layers) from image summaries.\n",
    "2. Frame the query as: \"What do [annotations] signify in [diagram type]?\"\n",
    "3. The answer MUST cite explicit descriptions (e.g., \"Arrows denote data flow\").\n",
    "\n",
    "Strict Rules:\n",
    "- Never interpret unlabeled elements.\n",
    "- Reject queries without annotation context or unrelated to Computer Science.\n",
    "- Either make queries general enough for multiple papers to possibly answer it, or make sure its clear which system/ model/ feature/ phenomenon is asked for! \n",
    "\n",
    "Examples:\n",
    "Context: \"Layers are labeled 'Conv1', 'Pool1' in the CNN diagram.\"\n",
    "Query: \"How are convolutional layers annotated in neural network diagrams?\"\n",
    "Answer: \"Labeled as 'Conv1', 'Conv2', etc.\"\n",
    "text_sentence: \"N/A\"\n",
    "image_sentence: \"Layers are labeled 'Conv1', 'Pool1' in the CNN diagram.\"\n",
    "Context: \"Red dashed lines in the flowchart indicate error handling.\"\n",
    "Query: \"What do red dashed lines represent in the workflow diagram?\"\n",
    "Answer: \"Error handling pathways.\"\n",
    "text_sentence: \"N/A\"\n",
    "image_sentence: \"Red dashed lines in the flowchart indicate error handling.\"'''\n",
    "\n",
    "## cross-modal \n",
    "\n",
    "process_explanation_prompt = \"\"\"Generate a query about a workflow. Follow these steps:\n",
    "Identify the process (e.g., federated learning, gradient clipping) from text and image summaries relating to the domain of Computer Science.\n",
    "Frame the query as: 'How does [process] work in [domain/ system]?'\n",
    "The answer MUST fuse modalities (include appropriate info from both modalities).\n",
    "If context is incomplete, state: 'Insufficient data to explain fully'.\n",
    "Strict Rules:\n",
    "Never force synthesis if modalities conflict.\n",
    "Reject queries that do not relate to the domain of Computer Science.\n",
    "Example:\n",
    "Context:\n",
    "Text: 'Residual connections bypass layers to mitigate vanishing gradients.'\n",
    "Image: 'Diagram shows skip paths around convolutional blocks.'\n",
    "Query: 'How do residual connections prevent vanishing gradients?'\n",
    "Answer: 'Skip paths (diagram) allow gradients to bypass layers (text).'\n",
    "text_sentence: 'Residual connections bypass layers to mitigate vanishing gradients.'\n",
    "image_sentence: 'Diagram shows skip paths around convolutional blocks.'\"\"\"\n",
    "\n",
    "conceptual_explanation_prompt = \"\"\"Generate a query explaining a concept. Follow these steps:\n",
    "Identify the concept (e.g., self-attention) from text and diagrams relating to the domain of Computer Science.\n",
    "Frame the query as: 'Explain [concept] in the context of [domain/ system].'\n",
    "The answer MUST link definitions to visuals (e.g., 'Q/K/V matrices in text → parallel heads in diagrams').\n",
    "If links are unclear, state: 'Visual evidence is incomplete'.\n",
    "Strict Rules:\n",
    "Reject queries that do not relate to the domain of Computer Science.\n",
    "Only assert connections explicitly described.\n",
    "Example:\n",
    "Context:\n",
    "Text: 'Self-attention computes token interactions via Q/K/V matrices.'\n",
    "Image: 'Diagram shows parallel attention heads.'\n",
    "Query: 'Explain how self-attention works in transformers.'\n",
    "Answer: 'Q/K/V matrices (text) process inputs through parallel heads (diagram).'\n",
    "text_sentence: 'Self-attention computes token interactions via Q/K/V matrices.'\n",
    "image_sentence: 'Diagram shows parallel attention heads.'\"\"\"\n",
    "\n",
    "\n",
    "conceptual_alignment_prompt = '''Generate a query comparing textual representations to visual data. Follow these steps:\n",
    "1. Identify a concept which describes a visual phenomenon (e.g., self-attention).\n",
    "2. Frame the query as: \"How do textual descriptions fit the visual representations of [concept]?\"\n",
    "3. The answer MUST map text (e.g., \"Q/K/V matrices\") to visuals (e.g., \"parallel heads\").\n",
    "\n",
    "Strict Rules:\n",
    "- Never assert alignment without explicit evidence.\n",
    "- Reject queries lacking multi-modal context or unrelated to Computer Science.\n",
    "\n",
    "Examples:\n",
    "Context: \n",
    "Text: \"Consensus algorithms require leader election and log replication phases.\"\n",
    "Image: \"State transition diagram shows candidate → leader → follower states.\"\n",
    "\n",
    "Query: \"How do textual and visual representations of consensus algorithms align?\"\n",
    "Answer: \"Leader election precedes log replication, with node states progressing through candidate, leader, and follower phases during election cycles.\"\n",
    "text_sentence: \"Consensus algorithms require leader election and log replication phases.\"\n",
    "image_sentence: \"State transition diagram shows candidate → leader → follower states.\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee429e02-aac1-49f0-9b91-5186b0c35bed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca8ec5-08ab-4d93-9a0c-b7ace5bff61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for generating prompts for figure images specifically another prompt:\n",
    "NEW_IMAGE_EVAL_PROMPT = \"\"\"Analyze COMPUTER SCIENCE image summaries to create retrieval-friendly queries. Follow these steps:\n",
    "\n",
    "1. CATEGORY SELECTION & GUIDANCE:\n",
    "[1] General Visualization (Typical Representations)\n",
    "- Query Patterns: \"How are [CS concepts] typically visualized...\", \"What diagram format shows [phenomenon]...\"\n",
    "- Example:  \n",
    "Context: \"Neural architecture diagram\"\n",
    "Query: \"How are attention mechanisms typically diagrammed?\"\n",
    "Answer: \"Usually shown as multi-head blocks with query/key/value matrices\"\n",
    "\n",
    "2 Specific Retrieval (Unique Elements)\n",
    "- Query Patterns: \"Show a [diagram type] of [system] with [features]\", \"Display the [chart] comparing [metrics]...\"\n",
    "- Example:\n",
    "Context: \"Blockchain consensus flowchart\"\n",
    "Query: \"Show a flowchart of PBFT consensus with prepare/commit phases\"\n",
    "Answer: \"PBFT diagram showing client request → pre-prepare → prepare → commit\"\n",
    "\n",
    "3 Data Patterns (Visualization Conventions)\n",
    "- Query Patterns: \"What chart type displays [metric] relationships?\", \"How is [phenomenon] visualized over [time]...\"\n",
    "- Example:\n",
    "Context: \"Latency boxplots across regions\"\n",
    "Query: \"What visualization shows statistical latency distributions?\"\n",
    "Answer: \"Boxplots comparing median latency and outliers per region\"\n",
    "\n",
    "4 Process Flows (System Sequences)\n",
    "- Query Patterns: \"What flowchart elements show [process] steps?\", \"Display workflow for [system] with [steps]...\"\n",
    "- Example:\n",
    "Context: \"CI/CD pipeline diagram\"\n",
    "Query: \"Show a deployment pipeline with testing and rollback steps\"\n",
    "Answer: \"Diagram shows code commit → test suite → staging → production\"\n",
    "\n",
    "5 Annotations (Symbol Conventions)\n",
    "- Query Patterns: \"What annotations indicate [function]?\", \"How are [elements] labeled in [diagram type]...\"\n",
    "- Example:\n",
    "Context: \"Red arrows in API diagram\"\n",
    "Query: \"What annotations show request flows in API diagrams?\"\n",
    "Answer: \"Arrows labeled with HTTP methods and endpoints\"\n",
    "\n",
    "6 Comparative (Contrasting Approaches)\n",
    "- Query Patterns: \"Compare visualization of [A] vs [B]\", \"How do diagrams differ between [X] and [Y]...\"\n",
    "- Example:\n",
    "Context: \"CNN/Transformer comparison figure\"\n",
    "Query: \"Compare layer representations in CNN vs transformer diagrams\"\n",
    "Answer: \"CNNs show convolution kernels, transformers show attention matrices\"\n",
    "\n",
    "7 Unsuitable (Non-CS/Unclear)\n",
    "\n",
    "2. QUERY RULES:\n",
    "- MUST be answerable from image summary alone\n",
    "- \n",
    "- Either:\n",
    "  a) General: Usable across papers (\"How are... typically\")\n",
    "  b) Specific: Unique to context (\"Show [exact topic and context]\"), but NOT detailed plot description  \n",
    "- NEVER assume prior article knowledge\n",
    "\n",
    "3. OUTPUT FORMAT:\n",
    "chosen_category: [1-7]\n",
    "query: [precise visual question]\n",
    "answer: [elements from summary]\n",
    "text_sentence: \"N/A\"\n",
    "image_sentence: [EXACT QUOTE FROM MOST FITTING CONTEXT]\n",
    "\n",
    "Now analyze:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e930e7a-be0a-432a-936f-c40d45a7327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FINAL COMBINED PROMPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe91457-deb2-47c3-beb2-6252414c11ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_EVAL_PROMPT = f\"\"\"Analyze this COMPUTER SCIENCE text context and:\n",
    "\n",
    "1. CATEGORY SELECTION - Choose the MOST appropriate text-based category:\n",
    "1. Numerical/Quantitative - Metrics/trends (accuracy, FLOPs, energy)\n",
    "2. Factual/Definitional - Formal CS definitions\n",
    "3. Compound Analysis - Multi-component systems\n",
    "4. Research Trends - Variable relationships\n",
    "5. Compare/Contrast - Method tradeoffs  \n",
    "6. Limitation Identification - System constraints\n",
    "7. Unsuitable - Non-CS or incomplete\n",
    "\n",
    "2. INSTRUCTIONS - Use corresponding prompt:\n",
    "1. {numerical_prompt}\n",
    "2. {factual_prompt}\n",
    "\n",
    "4. {trend_analysis_prompt}\n",
    "5. {compare_prompt}\n",
    "6. {limitation_identification_prompt}\n",
    "7. Unsuitable\n",
    "\n",
    "3. GENERATION RULES:\n",
    "- Strict CS focus (ML, systems, security)\n",
    "- Queries must NOT require visual analysis\n",
    "- Generate queries that are not too specific for the given article.\n",
    "- Reject speculative or multi-modal questions\n",
    "\n",
    "4. OUTPUT FORMAT:\n",
    "chosen_category: [1-7]\n",
    "query: [CS question answerable through text]\n",
    "answer: [technical answer from text context]\n",
    "text_sentence: [exact quote | \"N/A\"]      #MAKE SURE THIS IS A DIRECT QUOTE FROM THE CONTEXT\n",
    "image_sentence: \"N/A\"\n",
    "\n",
    "Example:\n",
    "chosen_category: 4  \n",
    "query: How does model width impact training stability in transformers?\n",
    "answer: Wider models show 38% lower gradient variance but require careful initialization.\n",
    "text_sentence: \"Width-1024 layers exhibit σ²=0.12 gradients vs σ²=0.19 for width-512 (Table 3).\"   #MAKE SURE THIS IS A DIRECT QUOTE FROM THE CONTEXT\n",
    "image_sentence: \"N/A\"\n",
    "\n",
    "Now process:\"\"\"\n",
    "\n",
    "IMAGE_EVAL_PROMPT = f\"\"\"Analyze COMPUTER SCIENCE image summaries and:\n",
    "\n",
    "1. CATEGORY SELECTION - Choose visual analysis category:\n",
    "1. Design Features - Architecture components\n",
    "2. Visual ID - Diagram elements\n",
    "3. Data Trends - Metric visualizations\n",
    "4. Image Retrieval - Visualization types  \n",
    "5. Functional Flow - Process sequences\n",
    "6. Annotation Analysis - Diagram symbology\n",
    "7. Unsuitable - Non-CS or unclear\n",
    "\n",
    "2. INSTRUCTIONS - Use corresponding prompt:\n",
    "1. {feature_enumeration_prompt}\n",
    "2. {visual_identification_prompt}\n",
    "3. {data_interpretation_prompt}\n",
    "4. {image_retrieval_prompt}\n",
    "5. {functional_flow_prompt}\n",
    "6. {annotation_prompt}\n",
    "7. Unsuitable\n",
    "\n",
    "3. GENERATION RULES:\n",
    "- Purely visual analysis (no text references)\n",
    "- Require explicit elements from image summaries\n",
    "- Reject queries needing textual explanations\n",
    "- MAKE SURE THE QUERY MENTIONS THE SPECIFIC SYTEM/ ARCHITECTURE/ METRIC\n",
    "\n",
    "4. OUTPUT FORMAT:  \n",
    "chosen_category: [1-7]\n",
    "query: [visual analysis question]\n",
    "answer: [elements from image summaries]\n",
    "text_sentence: \"N/A\"\n",
    "image_sentence: [exact image summary quote]      #MAKE SURE THIS IS A DIRECT QUOTE FROM THE CONTEXT\n",
    "\n",
    "Example:\n",
    "chosen_category: 2\n",
    "query: What layers are shown in the neural architecture diagram?\n",
    "answer: Input embedding layer, four transformer blocks, classification head.\n",
    "image_sentence: \"Diagram labels: Embedding Layer → Transformer Block ×4 → CLS Head\"\n",
    "\n",
    "Now process:\"\"\"\n",
    "\n",
    "CROSS_MODAL_EVAL_PROMPT = f\"\"\"Analyze BOTH text and images in this CS context and:\n",
    "\n",
    "1. CATEGORY SELECTION - Choose integration category:\n",
    "1. Method Validation - Technique-result alignment\n",
    "2. Process Explanation - Workflow integration  \n",
    "3. Conceptual Synthesis - Theory-visual links\n",
    "4. Representation Alignment - Text-diagram consistency\n",
    "5. Unsuitable - Single-modality sufficient\n",
    "\n",
    "2. INSTRUCTIONS - Use corresponding prompt:\n",
    "1. {process_explanation_prompt}\n",
    "2. {conceptual_explanation_prompt}\n",
    "\n",
    "4. Unsuitable\n",
    "\n",
    "3. GENERATION RULES:\n",
    "- MUST require both modalities to answer\n",
    "- Bridge text and visual elements\n",
    "- Flag modality conflicts\n",
    "- Reject questions answerable with one source\n",
    "\n",
    "4. OUTPUT FORMAT:\n",
    "chosen_category: [1-4]  \n",
    "query: [multi-modal CS question]\n",
    "answer: [synthesis of both sources]\n",
    "text_sentence: [relevant text quote | \"N/A\"]      #MAKE SURE THIS IS A DIRECT QUOTE FROM THE TEXT CONTEXT\n",
    "image_sentence: [relevant image quote | \"N/A\"]     #MAKE SURE THIS IS A DIRECT QUOTE FROM THE IMAGE CONTEXT\n",
    "\n",
    "Example:\n",
    "chosen_category: 3\n",
    "query: How do textual descriptions of attention mechanisms align with their visual representations?\n",
    "answer: Text describes parallel attention heads processing Q/K/V vectors (Vaswani 2017), while diagrams show eight-headed blocks with interleaved connections.\n",
    "text_sentence: \"Multi-head attention enables parallel processing of different representation subspaces.\"\n",
    "image_sentence: \"Figure 2: 8 attention heads with cross-connecting weights.\"\n",
    "\n",
    "Now process:\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86895d-0df5-4b36-889d-ecd0d8c18424",
   "metadata": {},
   "source": [
    "### Generating Query & Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197dd22e-7401-4170-9377-799092e5330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rndm_key = get_rndm_key()\n",
    "all_contexts = get_contexts_for_key(rndm_key)\n",
    "rndm_full_text = all_contexts['full_text'][0]\n",
    "rndm_sumaries = all_contexts['image_descriptions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bac957-f527-4839-8769-da2fabff8402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-only\n",
    "gene_query_t = generate_test_query(\n",
    "    instruction=TEXT_EVAL_PROMPT,\n",
    "    text_context=rndm_full_text,\n",
    "    #image_descriptions=rndm_sumaries\n",
    ")\n",
    "print(gene_query_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03afd866-be27-41a0-a8d3-e53e312a01b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summaries-only\n",
    "gene_query_i = generate_test_query(\n",
    "    instruction=NEW_IMAGE_EVAL_PROMPT,\n",
    "    #text_context=rndm_full_text,\n",
    "    image_descriptions=rndm_sumaries\n",
    ")\n",
    "print(gene_query_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0129455-0d6a-434e-913d-36d89ea09df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-modal\n",
    "gene_query = generate_test_query(\n",
    "    instruction=CROSS_MODAL_EVAL_PROMPT,\n",
    "    text_context=rndm_full_text,\n",
    "    image_descriptions=rndm_sumaries\n",
    ")\n",
    "print(gene_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f0fd47-afd1-499c-87ae-95b275a432e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#can be replaced with actual 4o call to extend with GT directly instead of first filtering the response of first call\n",
    "augmented_df_one = augment_with_ground_truth(all_chunks_df, captions,one_new_df)   \n",
    "#augmented_df_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fa914c-7eb7-4a24-ad92-8e3cd0a2428e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "augmented_df_one['type'] = 'cross-modal'    #'text-only' #'image-only' #'cross-modal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66baa14-4ceb-45c1-877b-2fa71e57e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second call to get optimal GT elements "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "final_df = process_dataframe_final(df_new)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "375b81b25ba29f31"
  },
  {
   "cell_type": "markdown",
   "id": "a2ed16b5-dd5c-4e3b-ba32-d7ec6bd48ec7",
   "metadata": {},
   "source": [
    "###  code: functions and helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba47194a-534a-4eb9-a7f1-505cc573da73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions needed to run to the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1509014-0641-4c52-bada-1b60bd197c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import time\n",
    "from typing import Optional, List\n",
    "\n",
    "def generate_test_query(\n",
    "    instruction: str,\n",
    "    text_context: Optional[str] = None,\n",
    "    image_descriptions: Optional[List[str]] = None\n",
    ") -> str:\n",
    "    content = []\n",
    "    text_parts = [\n",
    "    \"INSTRUCTION: Generate a test query and the according reference answer for a multi-modal RAG system FOR THE DOMAIN OF COMPUTER SCIENCE based on:\",\n",
    "    f\"- User instruction: {instruction}\",\n",
    "    \"CONTEXT GROUND TRUTH:\"\n",
    "    ]\n",
    "    \n",
    "    if text_context:\n",
    "        text_parts.append(f\"Full article text: {text_context}\")\n",
    "    \n",
    "    if image_descriptions:\n",
    "        text_parts.append(\"Image descriptions from article:\")\n",
    "        for i, desc in enumerate(image_descriptions, 1):\n",
    "            text_parts.append(f\"{i}. {desc}\")\n",
    "    \n",
    "    content.append({\n",
    "        \"type\": \"text\",\n",
    "        \"text\": \"\\n\".join(text_parts)\n",
    "    })\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": content}],\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def ask_4o(row):\n",
    "    cur_key = row['key']\n",
    "    cur_query = row['query']\n",
    "    cur_answer = row['answer']\n",
    "    type_ = row['type']\n",
    "    \n",
    "    both_contexts = get_sorted_contexts_for_key(cur_key)\n",
    "    text_chunks = both_contexts.get('text_chunks', [])\n",
    "    img_summaries = both_contexts.get('image_contexts', [])\n",
    "    \n",
    "    text_context_str = \"\\n\".join([f\"{idx}: {chunk}\" for idx, chunk in enumerate(text_chunks)])\n",
    "    img_context_str = \"\\n\".join([f\"Path: {img['image_path']}\\nSummary: {img['summary']}\" for img in img_summaries])\n",
    "    \n",
    "    base_instruction = \"\"\"Analyze this computer science query and reference answer which will be used\n",
    "    for testing a retrieval system. You MUST follow these rules:\n",
    "    1. Be strictly factual and objective\n",
    "    2. Only select items directly supporting both query AND answer\n",
    "    3. Never invent or guess information\n",
    "    4. If multiple candidates exist, choose the BEST match using cross-modal understanding\n",
    "    \n",
    "    QUERY: {query}\n",
    "    ANSWER: {answer}\n",
    "    \"\"\"\n",
    "    \n",
    "    if type_ == 'image':\n",
    "        final_prompt = f\"\"\"{base_instruction}\n",
    "        \n",
    "        IMAGE SUMMARIES FROM SOURCE ARTICLE:\n",
    "        {img_context_str}\n",
    "        \n",
    "        {EX_OUTPUT_image}\n",
    "        \"\"\"\n",
    "        \n",
    "    elif type_ == 'text':\n",
    "        final_prompt = f\"\"\"{base_instruction}\n",
    "        \n",
    "        TEXT CHUNKS FROM SOURCE ARTICLE:\n",
    "        {text_context_str}\n",
    "        \n",
    "        {EX_OUTPUT_text}\n",
    "        \"\"\"\n",
    "        \n",
    "    elif type_ == 'cross-modal':\n",
    "        final_prompt = f\"\"\"{base_instruction}\n",
    "        \n",
    "        TEXT CHUNKS FROM SOURCE ARTICLE:\n",
    "        {text_context_str}\n",
    "        \n",
    "        IMAGE SUMMARIES FROM SOURCE ARTICLE:\n",
    "        {img_context_str}\n",
    "        \n",
    "        {EX_OUTPUT_both}\n",
    "        \"\"\"\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid type: {type_}\")\n",
    "    \n",
    "    final_prompt = final_prompt.format(query=cur_query, answer=cur_answer)\n",
    "    \n",
    "    final_prompt += \"\\n\\nYOUR RESPONSE MUST USE EXACTLY THIS FORMAT:\\n\" + \\\n",
    "        json.dumps({\"choices\": [{\"text_chunk_index\": int, \"image_path\": str}]}) + \\\n",
    "        \"\\nOnly include actually matched items!\"\n",
    "    \n",
    "    return call_4o_most_appr(final_prompt)\n",
    "\n",
    "def call_4o_most_appr(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": prompt}]}],\n",
    "        max_tokens=300,\n",
    "        temperature=0.1 \n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        return json.loads(response.choices[0].message.content)\n",
    "    except json.JSONDecodeError:\n",
    "        return {\"error\": \"Invalid JSON response\", \"raw\": response.choices[0].message.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f42a33-fe6c-4fd2-b06f-993e856c956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "df_ten = pd.read_csv(\"data/tenk_subset.csv\")   #file with keys from a subset\n",
    "\n",
    "tenk_keys = df_ten[\"doc_id\"].unique().tolist()\n",
    "\n",
    "\n",
    "def get_rndm_key():\n",
    "    return random.choice(tenk_keys)\n",
    "\n",
    "#getting optimal GT elements to compare to previous elements and if match fine if not delete or choose more fitting one youself\n",
    "def process_dataframe_final(df):\n",
    "    \"\"\"Final debugged processor with proper string formatting\"\"\"\n",
    "    df = df.copy()\n",
    "    df['clean_type'] = df['type'].str.lower().str.strip() \\\n",
    "        .replace({'text-only': 'text', 'image-only': 'image', 'crossmodal': 'cross-modal'})\n",
    "    \n",
    "    valid_df = df[df['clean_type'].isin(['text', 'image', 'cross-modal'])].copy()\n",
    "    \n",
    "    \n",
    "    def get_prompt(row):\n",
    "        context = get_sorted_contexts_for_key(row['key'])\n",
    "        base = f\"\"\"Analyze this query and answer pair from computer science research papers and\n",
    "FIND THE MOST APPROPRIATE CONTEXT TO ANSWER THE QUESTION.\n",
    "Query: {row['query']}\n",
    "Answer: {row['answer']}\n",
    "\n",
    "You MUST respond in EXACTLY this format:\n",
    "\"\"\"\n",
    "        if row['clean_type'] == 'image':\n",
    "            images = \"\\n\".join([f\"- {img['image_path']}: {img.get('summary', '')}\"  \n",
    "                              for img in context.get('image_contexts', [])])\n",
    "            return f\"\"\"{base}image path: [exact_filename_from_below]\n",
    "Available images:\n",
    "{images}\"\"\"\n",
    "\n",
    "        elif row['clean_type'] == 'text':\n",
    "            chunks = \"\\n\".join([f\"{idx}: {chunk}\"\n",
    "                               for idx, chunk in enumerate(context.get('text_chunks', []))])\n",
    "            return f\"\"\"{base}chunk index: [number_from_below]\n",
    "Available chunks:\n",
    "{chunks}\"\"\"\n",
    "\n",
    "        else:  #cross\n",
    "            images = \"\\n\".join([f\"- {img['image_path']}: {img.get('summary', '')}\"  \n",
    "                              for img in context.get('image_contexts', [])])\n",
    "            chunks = \"\\n\".join([f\"{idx}: {chunk}\"\n",
    "                               for idx, chunk in enumerate(context.get('text_chunks', []))])\n",
    "            return f\"\"\"{base}image path: [exact_filename]\n",
    "chunk index: [number]\n",
    "\n",
    "Available images:\n",
    "{images}\n",
    "\n",
    "Available chunks:\n",
    "{chunks}\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for idx, row in tqdm(valid_df.iterrows(), total=len(valid_df)):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": get_prompt(row)}],\n",
    "                temperature=0.1,\n",
    "                max_tokens=200\n",
    "            ).choices[0].message.content\n",
    "\n",
    "            image_path, chunk_index = None, None\n",
    "            for line in response.split('\\n'):\n",
    "                clean_line = line.lower().strip()\n",
    "                if 'image path:' in clean_line:\n",
    "                    image_path = clean_line.split(':')[-1].strip()\n",
    "                if 'chunk index:' in clean_line:\n",
    "                    chunk_index = clean_line.split(':')[-1].strip()\n",
    "\n",
    "            results.append({\n",
    "                'raw_response': response,\n",
    "                'image_path': image_path,\n",
    "                'chunk_index': int(chunk_index) if chunk_index and chunk_index.isdigit() else None\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'raw_response': f\"ERROR: {str(e)}\",\n",
    "                'image_path': None,\n",
    "                'chunk_index': None\n",
    "            })\n",
    "\n",
    "    return pd.concat([\n",
    "        valid_df.reset_index(drop=True),\n",
    "        pd.DataFrame(results)\n",
    "    ], axis=1)\n",
    "\n",
    "def fix_extracted_values_fast(df):\n",
    "    unique_keys = df['key'].unique()\n",
    "    context_cache = {}\n",
    "    \n",
    "    for key in unique_keys:\n",
    "        context = get_sorted_contexts_for_key(key)\n",
    "        image_paths = [img['image_path'] for img in context.get('image_contexts', [])]\n",
    "        text_chunks = context.get('text_chunks', [])\n",
    "        \n",
    "        context_cache[key] = {\n",
    "            'path_lookup': {path.lower(): path for path in image_paths},\n",
    "            'max_chunk_idx': len(text_chunks) - 1\n",
    "        }\n",
    "    \n",
    "    df = df.copy()\n",
    "    df['image_path'] = (\n",
    "        df.groupby('key', group_keys=False)['image_path']\n",
    "        .transform(lambda x: x.str.lower().map(context_cache[x.name]['path_lookup'])))\n",
    "    \n",
    "    df['chunk_index'] = (\n",
    "        df['chunk_index']\n",
    "        .astype(str)\n",
    "        .str.extract(r'(\\d+)', expand=False)\n",
    "        .astype('Int64')\n",
    "    )\n",
    "    \n",
    "    df['max_chunk'] = df['key'].map(lambda k: context_cache[k]['max_chunk_idx'])\n",
    "    df['chunk_index'] = df['chunk_index'].mask(\n",
    "        (df['chunk_index'].lt(0)) | \n",
    "        (df['chunk_index'].gt(df['max_chunk'])),\n",
    "        pd.NA\n",
    "    )\n",
    "    df = df.drop(columns=['max_chunk'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def fix_extracted_values_final(df):\n",
    "    path_maps = {}\n",
    "    for key in df['key'].unique():\n",
    "        context = get_sorted_contexts_for_key(key)\n",
    "        image_contexts = context.get('image_contexts', [])\n",
    "        path_maps[key] = {\n",
    "            img['path'].lower(): img['path'] \n",
    "            for img in image_contexts\n",
    "            if 'path' in img  \n",
    "        }\n",
    "    \n",
    "    df = df.copy()\n",
    "    if 'image_path' in df:\n",
    "        df['image_path'] = df.apply(\n",
    "            lambda row: path_maps.get(row['key'], {}).get(\n",
    "                str(row['image_path']).lower().strip(), \n",
    "                None  \n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    if 'chunk_index' in df:\n",
    "        df['chunk_index'] = (\n",
    "            df['chunk_index']\n",
    "            .astype(str)\n",
    "            .str.extract(r'(\\d+)', expand=False)  \n",
    "            .astype('Int64')  \n",
    "        )\n",
    "        \n",
    "        chunk_counts = {\n",
    "            key: len(get_sorted_contexts_for_key(key).get('text_chunks', []))\n",
    "            for key in df['key'].unique()\n",
    "        }\n",
    "        df['chunk_index'] = df.apply(\n",
    "            lambda row: (\n",
    "                row['chunk_index'] \n",
    "                if pd.notna(row['chunk_index']) and \n",
    "                   0 <= row['chunk_index'] < chunk_counts.get(row['key'], 0)\n",
    "                else None\n",
    "            ),\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def process_dataframe_final(df):\n",
    "\n",
    "    df = df.copy()\n",
    "    df['clean_type'] = df['type'].str.lower().str.strip() \\\n",
    "        .replace({'text-only': 'text', 'image-only': 'image', 'crossmodal': 'cross-modal'})\n",
    "    \n",
    "    valid_df = df[df['clean_type'].isin(['text', 'image', 'cross-modal'])].copy()\n",
    "    \n",
    "    def get_prompt(row):\n",
    "        context = get_sorted_contexts_for_key(row['key'])\n",
    "        base = f\"\"\"Analyze this query and answer pair from computer science research papers and\n",
    "FIND THE MOST APPROPRIATE CONTEXT TO ANSWER THE QUESTION.\n",
    "Query: {row['query']}\n",
    "Answer: {row['answer']}\n",
    "\n",
    "You MUST respond in EXACTLY this format:\n",
    "\"\"\"\n",
    "        if row['clean_type'] == 'image':\n",
    "            images = \"\\n\".join([f\"- {img['image_path']}: {img.get('summary', '')}\" \n",
    "                              for img in context.get('image_contexts', [])])\n",
    "            return f\"\"\"{base}image path: [exact_filename_from_below]\n",
    "Available images:\n",
    "{images}\"\"\"\n",
    "\n",
    "        elif row['clean_type'] == 'text':\n",
    "            chunks = \"\\n\".join([f\"{idx}: {chunk}\"\n",
    "                               for idx, chunk in enumerate(context.get('text_chunks', []))])\n",
    "            return f\"\"\"{base}chunk index: [number_from_below]\n",
    "Available chunks:\n",
    "{chunks}\"\"\"\n",
    "\n",
    "        else: \n",
    "            images = \"\\n\".join([f\"- {img['image_path']}: {img.get('summary', '')}\"  \n",
    "                              for img in context.get('image_contexts', [])])\n",
    "            chunks = \"\\n\".join([f\"{idx}: {chunk}\"\n",
    "                               for idx, chunk in enumerate(context.get('text_chunks', []))])\n",
    "            return f\"\"\"{base}image path: [exact_filename]\n",
    "chunk index: [number]\n",
    "\n",
    "Available images:\n",
    "{images}\n",
    "\n",
    "Available chunks:\n",
    "{chunks}\"\"\"\n",
    "\n",
    "    results = []\n",
    "    for idx, row in tqdm(valid_df.iterrows(), total=len(valid_df)):\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL,\n",
    "                messages=[{\"role\": \"user\", \"content\": get_prompt(row)}],\n",
    "                temperature=0.1,\n",
    "                max_tokens=200\n",
    "            ).choices[0].message.content\n",
    "\n",
    "            image_path, chunk_index = None, None\n",
    "            for line in response.split('\\n'):\n",
    "                clean_line = line.lower().strip()\n",
    "                if 'image path:' in clean_line:\n",
    "                    image_path = clean_line.split(':')[-1].strip()\n",
    "                if 'chunk index:' in clean_line:\n",
    "                    chunk_index = clean_line.split(':')[-1].strip()\n",
    "\n",
    "            results.append({\n",
    "                'raw_response': response,\n",
    "                'image_path': image_path,\n",
    "                'chunk_index': int(chunk_index) if chunk_index and chunk_index.isdigit() else None\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            results.append({\n",
    "                'raw_response': f\"ERROR: {str(e)}\",\n",
    "                'image_path': None,\n",
    "                'chunk_index': None\n",
    "            })\n",
    "\n",
    "    return pd.concat([\n",
    "        valid_df.reset_index(drop=True),\n",
    "        pd.DataFrame(results)\n",
    "    ], axis=1)\n",
    "\n",
    "#this includes the option to manually input info\n",
    "def parse_response_to_dataframe(response_str, key, existing_df=None):\n",
    "    responses = [response_str] if isinstance(response_str, str) else response_str\n",
    "    keys = [key] if isinstance(key, str) else key\n",
    "    \n",
    "    if len(responses) != len(keys):\n",
    "        raise ValueError(\"Number of responses and keys must match\")\n",
    "\n",
    "    pattern = r\"\"\"\n",
    "        chosen\\s*category:\\s*(?P<chosen_category>\\d+).*?\n",
    "        query:\\s*(?P<query>.*?)(?=\\s*answer:|$)\n",
    "        .*?answer:\\s*(?P<answer>.*?)(?=\\s*text_sentence:|$)\n",
    "        .*?text_sentence:\\s*(?P<text_sentence>.*?)(?=\\s*image_sentence:|$)\n",
    "        .*?image_sentence:\\s*(?P<image_sentence>.*?)(?=\\s*\\w+:|$)\n",
    "    \"\"\".replace('\\n', '')\n",
    "\n",
    "    parsed_data = []\n",
    "    \n",
    "    for resp, k in zip(responses, keys):\n",
    "        clean_resp = re.sub(r'(\\n\\s*)+', '\\n', resp.strip())\n",
    "        match = re.search(pattern, clean_resp, re.DOTALL | re.IGNORECASE | re.VERBOSE)\n",
    "        \n",
    "        if match:\n",
    "            entry = match.groupdict()\n",
    "            entry = {k: v.strip() if v else 'N/A' for k, v in entry.items()}\n",
    "            entry['key'] = k\n",
    "        else:\n",
    "            print(f\"\\nFailed to parse response. Please enter fields manually for key: {k}\")\n",
    "            print(\"Original text snippet:\", clean_resp[:200] + \"...\\n\")\n",
    "            \n",
    "            entry = {\n",
    "                'chosen_category': input(\"Category (1-9): \").strip(),\n",
    "                'query': input(\"Query: \").strip(),\n",
    "                'answer': input(\"Answer: \").strip(),\n",
    "                'text_sentence': input(\"Text sentence (or 'N/A'): \").strip(),\n",
    "                'image_sentence': input(\"Image sentence (or 'N/A'): \").strip(),\n",
    "                'key': k\n",
    "            }\n",
    "\n",
    "        try:\n",
    "            for field in ['text_sentence', 'image_sentence']:\n",
    "                entry[field] = re.sub(r'^[\"\\']|[\"\\']$', '', entry[field])\n",
    "\n",
    "            if not entry['chosen_category'].isdigit():\n",
    "                raise ValueError(\"Category must be a number 1-9\")\n",
    "            if int(entry['chosen_category']) not in range(1,10):\n",
    "                raise ValueError(\"Category must be between 1-9\")\n",
    "            if not entry['query'] or not entry['answer']:\n",
    "                raise ValueError(\"Query and Answer are required\")\n",
    "            if entry['text_sentence'] == 'N/A' and entry['image_sentence'] == 'N/A':\n",
    "                raise ValueError(\"At least one sentence (text or image) required\")\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(f\"\\nValidation error: {e}\")\n",
    "            print(\"Please fix the entry:\")\n",
    "            entry = {\n",
    "                'chosen_category': input(f\"Category [current: {entry['chosen_category']}]: \") or entry['chosen_category'],\n",
    "                'query': input(f\"Query [current: {entry['query']}]: \") or entry['query'],\n",
    "                'answer': input(f\"Answer [current: {entry['answer']}]: \") or entry['answer'],\n",
    "                'text_sentence': input(f\"Text sentence [current: {entry['text_sentence']}]: \") or entry['text_sentence'],\n",
    "                'image_sentence': input(f\"Image sentence [current: {entry['image_sentence']}]: \") or entry['image_sentence'],\n",
    "                'key': k\n",
    "            }\n",
    "\n",
    "        parsed_data.append(entry)\n",
    "\n",
    "    new_df = pd.DataFrame(parsed_data)[[\n",
    "        'chosen_category', 'query', 'answer', \n",
    "        'text_sentence', 'image_sentence', 'key'\n",
    "    ]]\n",
    "    new_df['chosen_category'] = pd.to_numeric(new_df['chosen_category'], errors='coerce')\n",
    "\n",
    "    if existing_df is not None:\n",
    "        if not isinstance(existing_df, pd.DataFrame):\n",
    "            raise TypeError(\"existing_df must be a pandas DataFrame\")\n",
    "        \n",
    "        required_columns = {'chosen_category', 'query', 'answer', \n",
    "                           'text_sentence', 'image_sentence', 'key'}\n",
    "        missing_cols = required_columns - set(existing_df.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"existing_df missing columns: {missing_cols}\")\n",
    "        \n",
    "        return pd.concat([existing_df, new_df], ignore_index=True)\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "\n",
    "def parse_response_to_dataframe(response_str, key, existing_df=None):\n",
    "    \"\"\"\n",
    "    Robust parser that handles markdown formatting and various field arrangements\n",
    "    \"\"\"\n",
    "    responses = [response_str] if isinstance(response_str, str) else response_str\n",
    "    keys = [key] if isinstance(key, str) else key\n",
    "    \n",
    "    if len(responses) != len(keys):\n",
    "        raise ValueError(\"Number of responses and keys must match\")\n",
    "\n",
    "    field_pattern = re.compile(\n",
    "        r'(?:^|\\n)\\s*[*#]*(chosen_category|query|answer|text_sentence|image_sentence)[*#]*[\\s:]+',\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    parsed_data = []\n",
    "    \n",
    "    for resp, k in zip(responses, keys):\n",
    "        clean_resp = re.sub(r'\\*\\*|#{2,}', '', resp)  \n",
    "        clean_resp = re.sub(r'\\n\\s+', '\\n', clean_resp) \n",
    "        clean_resp = re.sub(r'(\\w)\\n(\\w)', r'\\1 \\2', clean_resp) \n",
    "        clean_resp = clean_resp.strip()\n",
    "\n",
    "        fields = {}\n",
    "        last_field = None\n",
    "        for part in field_pattern.split(clean_resp):\n",
    "            part = part.strip()\n",
    "            if not part:\n",
    "                continue\n",
    "            \n",
    "            if part.lower() in ['chosen_category', 'query', 'answer', \n",
    "                               'text_sentence', 'image_sentence']:\n",
    "                last_field = part.lower()\n",
    "            elif last_field:\n",
    "                value = re.split(r'\\n\\s*(?=\\S+:)', part)[0]\n",
    "                value = re.sub(r'^[\"\\']|[\"\\']$', '', value.strip())\n",
    "                fields[last_field] = value\n",
    "                last_field = None\n",
    "\n",
    "        required = ['chosen_category', 'query', 'answer']\n",
    "        if not all(f in fields for f in required):\n",
    "            print(f\"\\nMissing fields in response for key {k}. Original snippet:\")\n",
    "            print(clean_resp[:200] + \"...\")\n",
    "            print(\"Please enter missing fields manually:\")\n",
    "            \n",
    "            fields = {\n",
    "                'chosen_category': input(f\"Category (1-9) [detected: {fields.get('chosen_category')}]: \") \n",
    "                                or fields.get('chosen_category'),\n",
    "                'query': input(f\"Query [detected: {fields.get('query')}]: \") \n",
    "                         or fields.get('query'),\n",
    "                'answer': input(f\"Answer [detected: {fields.get('answer')}]: \") \n",
    "                          or fields.get('answer'),\n",
    "                'text_sentence': input(f\"Text sentence [detected: {fields.get('text_sentence')}]: \") \n",
    "                             or fields.get('text_sentence', 'N/A'),\n",
    "                'image_sentence': input(f\"Image sentence [detected: {fields.get('image_sentence')}]: \") \n",
    "                              or fields.get('image_sentence', 'N/A'),\n",
    "            }\n",
    "\n",
    "        entry = {\n",
    "            'chosen_category': str(fields.get('chosen_category', '')).strip(),\n",
    "            'query': fields.get('query', '').replace('\\n', ' ').strip(),\n",
    "            'answer': fields.get('answer', '').replace('\\n', ' ').strip(),\n",
    "            'text_sentence': fields.get('text_sentence', 'N/A'),\n",
    "            'image_sentence': fields.get('image_sentence', 'N/A'),\n",
    "            'key': k\n",
    "        }\n",
    "\n",
    "        while True:\n",
    "            try:\n",
    "                if not entry['chosen_category'].isdigit():\n",
    "                    raise ValueError(\"Category must be a number\")\n",
    "                if int(entry['chosen_category']) not in range(1,10):\n",
    "                    raise ValueError(\"Category must be 1-9\")\n",
    "                if len(entry['query']) < 10 or len(entry['answer']) < 10:\n",
    "                    raise ValueError(\"Query/Answer too short\")\n",
    "                break\n",
    "            except ValueError as e:\n",
    "                print(f\"Validation error: {e}\")\n",
    "                entry['chosen_category'] = input(f\"Category (1-9) [current: {entry['chosen_category']}]: \") or entry['chosen_category']\n",
    "                entry['query'] = input(f\"Query [current: {entry['query']}]: \") or entry['query']\n",
    "                entry['answer'] = input(f\"Answer [current: {entry['answer']}]: \") or entry['answer']\n",
    "\n",
    "        parsed_data.append(entry)\n",
    "\n",
    "    new_df = pd.DataFrame(parsed_data)[[\n",
    "        'chosen_category', 'query', 'answer', \n",
    "        'text_sentence', 'image_sentence', 'key'\n",
    "    ]]\n",
    "    new_df['chosen_category'] = pd.to_numeric(\n",
    "        new_df['chosen_category'], \n",
    "        errors='coerce', \n",
    "        downcast='integer'\n",
    "    )\n",
    "\n",
    "    if existing_df is not None:\n",
    "        return pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates()\n",
    "    \n",
    "    return new_df\n",
    "\n",
    "\n",
    "def get_contexts_for_key(key):\n",
    "    full_text = df_full_texts[df_full_texts['key'] == key]['full_text'].tolist()\n",
    "\n",
    "    image_descriptions = image_summary_df[image_summary_df['doc_id'] == key]['image_summary'].tolist()\n",
    "\n",
    "    return {\n",
    "        'full_text': full_text,\n",
    "        'image_descriptions': image_descriptions\n",
    "    }\n",
    "\n",
    "\n",
    "def get_sorted_contexts_for_key(key):\n",
    "    text_chunks = all_chunks_df[all_chunks_df['doc_id'] == key] \\\n",
    "        .sort_values('chunk_index') \\\n",
    "        .apply(lambda row: {\n",
    "            'chunk_index': row['chunk_index'],\n",
    "            'chunk_content': row['text_content']\n",
    "        }, axis=1) \\\n",
    "        .tolist()\n",
    "\n",
    "    image_contexts = image_summary_df[image_summary_df['doc_id'] == key] \\\n",
    "        .apply(lambda row: {\n",
    "            'image_path': row['original_image_path'].split(\"/\")[-1], \n",
    "            'summary': row['image_summary']\n",
    "        }, axis=1) \\\n",
    "        .tolist()\n",
    "\n",
    "    return {\n",
    "        'text_chunks': text_chunks,\n",
    "        'image_contexts': image_contexts\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def process_image_path(path: str) -> str:\n",
    "    if not path.startswith('data/'):\n",
    "        path = f\"data/399k_imgs/{path.split('/')[-1]}\"\n",
    "    Image.open(path).show()\n",
    "    return \" \"\n",
    "\n",
    "def augment_with_ground_truth(df1: pd.DataFrame,\n",
    "                             df2: pd.DataFrame,\n",
    "                             df3: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    \n",
    "    df2 = df2.copy()\n",
    "    df2['doc_key'] = df2['image_path'].str.extract(r'.*/(.*?)_', expand=False)\n",
    "    \n",
    "    text_lookup = df1.groupby('doc_id')['text_content'].agg(list).to_dict()\n",
    "    image_lookup = (df2.groupby('doc_key')\n",
    "                    .apply(lambda x: list(zip(x['image_path'], x['generated_caption'])))\n",
    "                    .to_dict())\n",
    "\n",
    "    def find_text_matches(row):\n",
    "        if pd.isna(row['text_sentence']) or row['text_sentence'] == 'N/A':\n",
    "            return []\n",
    "        \n",
    "        chunks = text_lookup.get(row['key'], [])\n",
    "        return [\n",
    "            idx for idx, content in enumerate(chunks)\n",
    "            if row['text_sentence'][:25] in content\n",
    "        ]\n",
    "    \n",
    "    def find_image_matches(row):\n",
    "        if pd.isna(row['image_sentence']) or row['image_sentence'] == 'N/A':\n",
    "            return []\n",
    "        \n",
    "        images = image_lookup.get(row['key'], [])\n",
    "        return [\n",
    "            f\"{path}\" #: {caption}\n",
    "            for path, caption in images\n",
    "            if row['image_sentence'][:25] in caption\n",
    "        ]\n",
    "    \n",
    "    df3 = df3.copy()\n",
    "    df3['gt_chunk_index'] = df3.apply(find_text_matches, axis=1)\n",
    "    df3['gt_summary'] = df3.apply(find_image_matches, axis=1)\n",
    "    \n",
    "    return df3\n",
    "\n",
    "def enrich_with_context(df):\n",
    "    df = df.copy()\n",
    "    df['text_content'] = None\n",
    "    df['image_summary'] = None\n",
    "    df['processed_image'] = None\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        key = row['key']\n",
    "        context = get_sorted_contexts_for_key(key)\n",
    "        \n",
    "        if pd.notna(row.get('chunk_index')):\n",
    "            try:\n",
    "                chunk = next(c for c in context['text_chunks'] \n",
    "                           if c['chunk_index'] == row['chunk_index'])\n",
    "                df.at[idx, 'text_content'] = chunk['chunk_content']\n",
    "            except (StopIteration, KeyError):\n",
    "                pass\n",
    "        \n",
    "        if pd.notna(row.get('image_path')):\n",
    "            try:\n",
    "                img = next(i for i in context['image_contexts']\n",
    "                         if i['image_path'] == row['image_path'])\n",
    "                df.at[idx, 'image_summary'] = img['summary']\n",
    "            except (StopIteration, KeyError):\n",
    "                pass\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### parse output\n",
    "#one_new_df = parse_response_to_dataframe(gene_query_t, rndm_key)   #, new_df\n",
    "#one_new_df = parse_response_to_dataframe(gene_query_i, rndm_key)   #, new_df\n",
    "one_new_df = parse_response_to_dataframe(gene_query, rndm_key)   #, new_df"
   ],
   "metadata": {
    "scrolled": true
   },
   "id": "c80334e9-83e3-410b-b1d0-f5c0a99f09d4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29fee43-0db0-48e6-b7f1-2aa7b6b600cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
