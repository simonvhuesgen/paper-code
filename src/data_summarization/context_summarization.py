import pandas as pd
import torch
from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline
from langchain_core.messages import HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import AzureChatOpenAI
from langchain.schema.runnable import RunnableLambda
from transformers import pipeline, LlavaNextForConditionalGeneration, LlavaNextProcessor
from tqdm.auto import tqdm
from typing import List, Tuple
from utils.azure_config import get_azure_config
from utils.base64_utils.base64_utils import *
from utils.model_loading_and_prompting.llava import format_prompt_with_image, llava_call
from rag_env import INPUT_DATA, IMG_SUMMARIES_CACHE_DIR, TEXT_SUMMARIES_CACHE_DIR



class TextSummarizer:
    """  
    A class to summarize texts using either AzureOpenAI's model or the LLama 3 8B model.
  
    Attributes:
        model_type (str): Type of the model to use for summarization.
        cache_path (str): Path to the directory where summaries will be cached as a CSV file.
        model (AzureChatOpenAI or HuggingFacePipeline): The summarization model loaded based on `model_type`.
        cache_file (str): The complete path to the cached CSV file containing text summaries.
        df (pd.DataFrame): DataFrame to store and manage texts and their corresponding summaries.
    """    
    
    def __init__(self, model_type: str, cache_path: str):
        """  
        Initializes the TextSummarizer object.
  
        :param model_type: The type of model to be used for summarization.
        :param cache_path: The directory path where the summaries will be cached.
        """ 
        self.model_type = model_type
        config = get_azure_config()
        
        if model_type in config:
            azure_llm_config = config[model_type]
            self.model = AzureChatOpenAI(
                openai_api_version=azure_llm_config["openai_api_version"],
                azure_endpoint=azure_llm_config["openai_endpoint"],
                azure_deployment=azure_llm_config["deployment_name"],
                model=azure_llm_config["model_version"],
                api_key=os.environ.get("GPT4V_API_KEY"),
                max_tokens=400)
        else:
            pipe = pipeline("text-generation",
                        model="meta-llama/Meta-Llama-3-8B-Instruct",
                        model_kwargs={"torch_dtype": torch.bfloat16},
                        device_map="auto")

            self.model = HuggingFacePipeline(pipeline=pipe)
            
        # Load cached DataFrame if it exists
        self.cache_file = os.path.join(cache_path, f'text_summaries_{self.model_type}.csv')
        if os.path.exists(self.cache_file):
            self.df = pd.read_csv(self.cache_file)
        else:
            # Initialize DataFrame if it doesn't exist
            self.df = pd.DataFrame(columns=['text', 'text_summary'])
            self.df.to_csv(self.cache_file, index=False)
    
    
    def summarize(self, texts: List[str]) -> List[str]:
        """  
        Generates summaries for a list of texts.
        This method determines which model to use based on the `model_type` attribute,
        and then calls the appropriate summarization method.
  
        :param texts: A list of texts to be summarized.
        :return: A list of summarized texts.
        """  
        if type(self.model) == AzureChatOpenAI:
            text_summaries = self.summarize_azure(texts)
        else:
            text_summaries = self.summarize_llama(texts)
        return text_summaries
    
    
    def summarize_llama(self, texts: List[str]) -> List[str]:
        """  
        Summarizes texts using the LLama 3 8B model.
        Iterates over the list of texts, checks if a summary already exists in the cache,
        and if not, generates a new summary using the LLama 3 8B model. Updates the cache
        with new summaries.
  
        :param texts: A list of texts to be summarized.
        :return: A list of summaries generated by the LLama model.
        """
  
        # Iterate over texts and generate summaries with a progress bar
        with tqdm(total=len(texts), desc="Summarizing texts") as pbar:
            for i, text in enumerate(texts):
                self.df.at[i, 'text'] = text

                pbar.update(1)
  
                # Skip if summary already exists
                if i < len(self.df) and pd.notna(self.df.at[i, 'text_summary']):
                    print(f"Summary for text {i + 1} already exists. Skipping...")
                    continue
                
                template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>{system_msg}
                        <|start_header_id|>user<|end_header_id|>Text: {text}\nSummary:\n<|eot_id|>
                        <|start_header_id|>assistant<|end_header_id|>"""

                system = """You are an assistant tasked with summarizing text for retrieval.
                    These summaries will be embedded and used to retrieve the raw text elements.
                    Give a concise summary of the text that is well optimized for retrieval.
                    Only output the summary, no additional explanation.\n"""

                prompt = PromptTemplate.from_template(template)
                prompt = prompt.partial(system_msg=system)

                print("Summarizing texts...")
                
                # Remove the text given to the model as input from the generation
                strip_output = RunnableLambda(lambda output: output.split("<|start_header_id|>assistant<|end_header_id|>",1)[1].strip())

                # Text summary chain
                summarize_chain = {"text": lambda text: text} | prompt | self.model | strip_output | StrOutputParser()
                summary = summarize_chain.invoke({"text": text})

                # Update DataFrame with new summary
                self.df.at[i, 'text_summary'] = summary
                # Cache the DataFrame after each generation
                self.df.to_csv(self.cache_file, index=False)
  
        return self.df['text_summary'].tolist()
    
    
    def summarize_azure(self, texts: List[str]) -> List[str]:
        """
        Summarizes texts using the Azure model.
        Iterates over the list of texts, checks if a summary already exists in the cache,
        and if not, generates a new summary using the Azure model. Updates the cache
        with new summaries.
  
        :param texts: A list of texts to be summarized.
  
        :return: A list of summaries generated by the Azure model.
        """
        print("Summarizing texts with Azure")
  
        # Iterate over texts and generate summaries with a progress bar
        with tqdm(total=len(texts), desc="Summarizing texts") as pbar:
            for i, text in enumerate(texts):
                self.df.at[i, 'text'] = text

                pbar.update(1)
  
                # Skip if summary already exists
                if i < len(self.df) and pd.notna(self.df.at[i, 'text_summary']):
                    print(f"Summary for text {i + 1} already exists. Skipping...")
                    continue
  
                # Prompt template
                prompt = f"""You are an assistant tasked with summarizing text for retrieval.
                These summaries will be embedded and used to retrieve the raw text element.
                Give a concise summary of the text that is well optimized for retrieval.
                Text: {text}\n"""
  
                try:
                    print(f"Summarizing text {i + 1} of {len(texts)}")
                    if self.model_type == "gpt4":
                        summary = self.model.invoke(
                            [
                                HumanMessage(
                                    content=prompt
                                )
                            ]
                        )
                        print(summary.content)
                except Exception as e:
                    print(f"Failed to summarize text {i}: {e}")
                    with open('summarization_fails.txt', 'a') as f:
                        f.write(f"Failed to summarize text {i}: {e}\n")
                    continue
  
                # Update DataFrame with new summary
                self.df.at[i, 'text_summary'] = summary.content
                # Cache the DataFrame after each generation
                self.df.to_csv(self.cache_file, index=False)
  
        return self.df['text_summary'].tolist()
    


class ImageSummarizer:
    """
    A class to summarize images using different models. It can encode images in base64,
    generate textual summaries, and cache these summaries for quick retrieval.
    """
    def __init__(self, model, tokenizer=None):
        """
        Initializes the ImageSummarizer with a specific model and an optional tokenizer.
          
        :param model: The model to be used for generating image summaries. This can be an instance of either
                      AzureChatOpenAI, LlavaNextForConditionalGeneration, or any model that supports image summarization.
        :param tokenizer: The tokenizer to be used with the model, if necessary. This is model-dependent and optional.
        """
        self.model = model
        self.tokenizer=tokenizer
        
        
    def summarize(self, image_bytes_list: List[bytes], cache_path: str) -> Tuple[List[str], List[str]]:
        """  
        Generate summaries and base64 encoded strings for images. This function also checks for cached summaries
        to avoid re-processing images. If a summary does not exist, it will generate a new one, update the cache,
        and return the summaries along with their base64 encoded strings.
          
        :param image_bytes_list: A list of image bytes. Each entry in the list should be the binary content of an image file.
        :param cache_path: The file system path where cached summaries are stored. This path is used to store summaries
                           in a CSV file to avoid re-processing images.
        :return: A tuple containing two lists - the first list contains the base64 encoded strings of the images,
                 and the second list contains the textual summaries of the images.
        """
        # Initialize base64 list
        img_base64_list = []

        if type(self.model) == AzureChatOpenAI:
            model_type = "gpt4v"
        else:
            model_type = "llava"
        # Load cached DataFrame if it exists
        cache_file = os.path.join(cache_path, f'image_summaries_{model_type}.csv')
        if os.path.exists(cache_file):
            df = pd.read_csv(cache_file)
        else:
            # Initialize DataFrame if it doesn't exist
            df = pd.DataFrame(columns=['image_summary'])
            df.to_csv(cache_file, index=False)

        # Prompt template
        prompt = """You are an assistant tasked with summarizing images for retrieval. \
                    These summaries will be embedded and used to retrieve the raw image. \
                    Give a concise summary of the image that is well optimized for retrieval."""

        # Iterate over image bytes and generate base64 encoded string
        for i, image_bytes in enumerate(image_bytes_list):
            # Convert image bytes to base64
            try:
                img_base64 = encode_image_from_bytes(image_bytes)
                img_base64_list.append(img_base64)
            except:
                print(f"Failed to encode img {i}...")
                continue

            # Skip if summary already exists
            if i < len(df):
                print(f"Summary for image {i + 1} already exists. Skipping...")
                continue
            try:
                print(f"Summarizing image {i + 1} of {len(image_bytes_list)}")
                if model_type == "gpt4v":  
                    summary_content = self.summarize_image_azure(img_base64, prompt)
                else:
                    summary_content = self.summarize_image_llava(img_base64, prompt)
            except:
                print(f"Failed to summarize img {i}")
                with open('summarization_fails.txt', 'a') as f:
                    f.write(f"Failed to summarize img {i}" + '\n')
                continue

            # Update DataFrame with new summary
            df.at[i, 'image_summary'] = summary_content

            # Cache the DataFrame after each generation
            df.to_csv(cache_file, index=False)

        return img_base64_list, df['image_summary'].tolist()
        
        
    def summarize_image_llava(self, img_base64: str, prompt: str) -> str:
        
        llava_prompt = format_prompt_with_image(prompt)
        image = decode_image_to_bytes(img_base64)
        image = Image.open(io.BytesIO(image))
        img_summary = llava_call(llava_prompt, self.model, self.tokenizer, device="cuda", image=image)

        return img_summary


    def summarize_image_azure(self, img_base64: str, prompt: str) -> str:

        msg = self.model.invoke(
            [
                HumanMessage(
                    content=[
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/jpeg;base64,{img_base64}"},
                        },
                    ]
                )
            ]
        )
        print(msg.content)

        return msg.content


if __name__ == "__main__":

    # text summarization
    #text_summarizer = TextSummarizer(model_type='llama3', cache_path=TEXT_SUMMARIES_CACHE_DIR)
    #df = pd.read_parquet(INPUT_DATA)
    #texts = list(df.drop_duplicates(subset='text')['text'])
    #text_summarizer.summarize(texts)

    # image summarization
    df = pd.read_parquet(INPUT_DATA)
    print(df.columns, df.shape)
    filtered_df = df[df['has_image'] == True]
    images = list(filtered_df["image_bytes"])
    
    model_id = "llava-hf/llava-v1.6-mistral-7b-hf"
    processor = LlavaNextProcessor.from_pretrained(model_id)
    model = LlavaNextForConditionalGeneration.from_pretrained(model_id, device_map="auto")
    
    image_summarizer = ImageSummarizer(model, processor)
    image_summarizer.summarize(images, cache_path=IMG_SUMMARIES_CACHE_DIR)
