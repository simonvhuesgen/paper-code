{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0368f2e-88fc-4f44-aac0-2d499484fd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load context dataset of all images\n",
    "#includes: keywords, mentions, original_caption, path to figure image and the full text\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "fs = s3fs.S3FileSystem(anon=False)\n",
    "s3_path = \"s3://___.parquet/\"\n",
    "\n",
    "files = fs.glob(f\"{s3_path}*.parquet\")\n",
    "print(f\"Found {len(files)} files in the directory.\")\n",
    "\n",
    "dfs = []\n",
    "for file in files:\n",
    "    df = pd.read_parquet(f\"s3://{file}\", filesystem=fs)\n",
    "    dfs.append(df)\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4809d4-17b7-4466-88ee-1387819b0e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00d35e9-646e-4c17-b866-a91c50a3f882",
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed to run captioning\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import math\n",
    "import datetime\n",
    "import time\n",
    "import concurrent.futures\n",
    "import warnings\n",
    "import multiprocessing as mp \n",
    "from itertools import chain\n",
    "from worker_utils_mult_GPU_context import inference_worker_batch    #loading and calling the model with batches (also pasted at the bottom of this notebook) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d5b07c-d916-4482-bdcf-068c2971e949",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_repo_prompt = \"\"\"You are an assistant tasked with summarizing images for retrieval. \\\n",
    "                    You are given contextual information as well as the image. \\\n",
    "                    These summaries will be embedded and used to retrieve the raw image. \\\n",
    "                    Give a concise summary of the image that is well optimized for retrieval.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94335d4f-2876-454d-ba50-223c792d2282",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt generation\n",
    "final_prompt = \"\"\"Analyze the scientific figure and any provided context to create a concise, retrieval-optimized caption. Follow these guidelines:\n",
    "\n",
    "Overview: Summarize the figureâ€™s main subject, purpose and experimental focus.\n",
    "\n",
    "Visual Details: Identify and describe important visual elements (e.g. charts, diagrams, symbols, numeric labels, legends).\n",
    "\n",
    "Relationship & Patterns: Explain interactions, trends, or hierarchies evident in the figure.\n",
    "\n",
    "Terminology & Keywords: Use relevant domain-specific terms (with plain-language equivalents if needed), then list essential keywords for retrieval.\n",
    "\n",
    "Relevance & Accuracy: Include only information visible in the figure or clearly stated in the context. Avoid speculation, repetition or irrelevant details.\n",
    "\n",
    "Formatting: Present the description coherently, in a paragraph of 300- to 400 words. Omit statements about missing elements unless explicitly relevant. \"\"\"\n",
    "\n",
    "\n",
    "# 0= original caption only, 1= title, abstract, keywords; 2: sentences, mention_section; 3: sentences, keywords, original caption\n",
    "\n",
    "\n",
    "def generate_prompt(row, config_num):   #\n",
    "    prompt_components = []\n",
    "\n",
    "    if config_num == 0:\n",
    "        prompt_components.append(f\"[CAPTION-START]{row['original_caption']}[CAPTION-END]\\n\")\n",
    "    elif config_num == 1:\n",
    "        prompt_components.append(f\"[TITLE-START]{ row['title']}[TITLE-END]\\n\")\n",
    "        prompt_components.append(f\"[ABSTRACT-START]{row['abstract']}[ABSTRACT-END]\\n\")\n",
    "        prompt_components.append(f\"[KEYWORDS-START]{row['keywords']}[KEYWORDS-END]\\n\")\n",
    "    elif config_num == 2:\n",
    "        sentences = row.get('sentences')\n",
    "        if hasattr(sentences, '__len__') and len(sentences) > 0:\n",
    "             if isinstance(sentences, (list, np.ndarray)):\n",
    "                  sentences_str = \" \".join(map(str, sentences)) \n",
    "             else:\n",
    "                  sentences_str = str(sentences)\n",
    "             prompt_components.append(f\"[MENTION-START]{sentences_str}[MENTION-END]\\n\")\n",
    "            \n",
    "        prompt_components.append(f\"[SECTION-START] This Figure is from the section: {row['mention_section']}[SECTION-END]\\n\")\n",
    "    elif config_num == 3:\n",
    "        sentences = row.get('sentences')\n",
    "        if hasattr(sentences, '__len__') and len(sentences) > 0:\n",
    "             if isinstance(sentences, (list, np.ndarray)):\n",
    "                  sentences_str = \" \".join(map(str, sentences)) \n",
    "             else:\n",
    "                  sentences_str = str(sentences)\n",
    "             prompt_components.append(f\"[MENTION-START]{sentences_str}[MENTION-END]\\n\")\n",
    "            \n",
    "        prompt_components.append(f\"[KEYWORDS-START]{row['keywords']}[KEYWORDS-END]\\n\")\n",
    "        prompt_components.append(f\"[CAPTION-START]{row['original_caption']}[CAPTION-END]\\n\")\n",
    "            \n",
    "    else:\n",
    "        print('INCORRECT CONFIG_NUM')\n",
    "\n",
    "    context_str = \"\".join(prompt_components)\n",
    "    final_prompt_instr = final_prompt\n",
    "\n",
    "    if context_str:\n",
    "        return context_str + \"\\n\" + final_prompt_instr\n",
    "    else:\n",
    "        return final_prompt_instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83909cc9-9a1c-489f-b173-973c735d28b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for multiGPU\n",
    "try:\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "    print(\"spawn set\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Warning: {e}\")\n",
    "\n",
    "\n",
    "if not torch.cuda.is_available():\n",
    "    raise SystemError(\"no CUDA!\")\n",
    "\n",
    "NUM_GPUS = torch.cuda.device_count() #- 2\n",
    "print(f\"Found {NUM_GPUS} GPUs\")\n",
    "if NUM_GPUS == 0:\n",
    "     raise SystemError(\"No GPUs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703b05b3-cc9d-48bb-8592-baaceb49543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#needed arguments/ params\n",
    "WORKER_BATCH_SIZE = 8\n",
    "MODEL_ID = \"openbmb/MiniCPM-V-2_6\"\n",
    "IMAGE_COLUMN_NAME = 'image_path'\n",
    "cache_path = 'cache/'\n",
    "os.makedirs(cache_path, exist_ok=True)\n",
    "model_type = 'miniCPM_testing_baseline_prmpt_REPO_run' #where to save\n",
    "caption_cache_file = os.path.join(cache_path, f'image_captions_{model_type}.csv')\n",
    "fail_log_file = os.path.join(cache_path, f'captioning_fails_{model_type}.txt')\n",
    "prompts_to_process_file = os.path.join(cache_path, f'prompts_to_process_{model_type}.jsonl')\n",
    "INPUT_DF_NAME = 'test_subset' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317078b6-d09e-4228-bb75-1a458a967df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data & set up cache to save captions\n",
    "df_input = test_subset.copy()\n",
    "if IMAGE_COLUMN_NAME not in df_input.columns:\n",
    "     print(f\"Need image paths avalable!\")\n",
    "\n",
    "\n",
    "processed_captions_list = []\n",
    "processed_image_paths = set()\n",
    "if os.path.exists(caption_cache_file):\n",
    "    print(\"Loading existing captions\")\n",
    "    try:\n",
    "        df_cache = pd.read_csv(caption_cache_file, usecols=['image_path', 'generated_caption', 'config_num'])\n",
    "        if not df_cache.empty:\n",
    "            cached_records = df_cache.apply(lambda x: (x['image_path'], x['config_num']), axis=1).tolist()\n",
    "            processed_image_paths.update(cached_records)\n",
    "        else:\n",
    "            print(\"Cache file exists but is empty.\")\n",
    "    except:\n",
    "        print(\"New cache.\")\n",
    "        processed_image_paths = set()\n",
    "else:\n",
    "    print(\"Starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4f7f00-12a7-4023-a6ed-42549da48ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating prompts first\n",
    "IMAGE_BASE_PATH = \"./data/399k_imgs/\"\n",
    "\n",
    "prompts_to_process_data = [] \n",
    "skipped_cached_count = 0\n",
    "skipped_invalid_count = 0\n",
    "invalid_paths_logged_phase2 = set()\n",
    "\n",
    "with open(fail_log_file, 'w') as f_fail:\n",
    "     f_fail.write(f\"Captioning Fail Log - Run Started: {datetime.datetime.now()}\\n\")\n",
    "     f_fail.write(\"--- Phase 2 Failures (Path/Prompt Gen) ---\\n\")\n",
    "\n",
    "print(\"generating prompts\")\n",
    "for index, row in tqdm(df_input.iterrows(), total=len(df_input), desc=\"Checking Images & Generating Prompts\"):\n",
    "    row_dict = row.to_dict()\n",
    "    row_index_for_log = index\n",
    "\n",
    "    relative_image_path = row_dict.get(IMAGE_COLUMN_NAME)\n",
    "    if not relative_image_path or pd.isna(relative_image_path):\n",
    "        log_key = f\"Row {row_index_for_log}: Missing Path\"\n",
    "        if log_key not in invalid_paths_logged_phase2:\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"Skipped row {row_index_for_log}: Missing image path in DataFrame\\n\")\n",
    "            invalid_paths_logged_phase2.add(log_key)\n",
    "        skipped_invalid_count += 1\n",
    "        continue\n",
    "\n",
    "    full_image_path = os.path.join(IMAGE_BASE_PATH, str(relative_image_path))\n",
    "    full_image_path = os.path.normpath(full_image_path)\n",
    "\n",
    "    if not os.path.exists(full_image_path):\n",
    "        log_key = f\"no file: {full_image_path}\"\n",
    "        if log_key not in invalid_paths_logged_phase2:\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"File not found at '{full_image_path}'\\n\")\n",
    "            invalid_paths_logged_phase2.add(log_key)\n",
    "        skipped_invalid_count += 1\n",
    "        continue\n",
    "\n",
    "\n",
    "    try:\n",
    "        for config_num in range(4):\n",
    "            prompt = generate_prompt(row_dict, config_num)\n",
    "            prompt_data = {\n",
    "                'image_path': full_image_path,\n",
    "                'prompt': prompt,\n",
    "                'config_num': config_num  \n",
    "            }\n",
    "            prompts_to_process_data.append(prompt_data)\n",
    "            with open(prompts_to_process_file, 'a') as f_prompts:\n",
    "                f_prompts.write(json.dumps(prompt_data) + '\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        log_key = f\"Prompt Gen Error: {full_image_path}\"\n",
    "        if log_key not in invalid_paths_logged_phase2:\n",
    "            print(f\"Failed to generate prompt for row {row_index_for_log}, image: {full_image_path}. Error: {e}\")\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"Failed row {row_index_for_log} during prompt generation: {full_image_path} | Error: {str(e)}\\n\")\n",
    "            invalid_paths_logged_phase2.add(log_key)\n",
    "        skipped_invalid_count += 1\n",
    "        continue\n",
    "\n",
    "num_prompts_to_process = len(prompts_to_process_data)\n",
    "print(f\"\\nIdentified {num_prompts_to_process} new images\")\n",
    "print(f\"Skipped {skipped_cached_count} in cache.\")\n",
    "print(f\"Skipped {skipped_invalid_count} rows\")\n",
    "if skipped_invalid_count > 0:\n",
    "     print(f\"(Check '{fail_log_file}' for details on failed items)\")\n",
    "if num_prompts_to_process > 0:\n",
    "    print(f\"Details of items to process saved to: {prompts_to_process_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d473f3-f419-44e2-b0f6-f13f94b11d76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#captioning code for any number GPUs\n",
    "if num_prompts_to_process == 0:\n",
    "    print(\"no new images.\")\n",
    "else:\n",
    "    print(f\"using {NUM_GPUS} GPUs\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if num_prompts_to_process < NUM_GPUS:\n",
    "         actual_num_workers = num_prompts_to_process\n",
    "         print(\"Warning: less items than GPUs\")\n",
    "         data_chunks = [[item] for item in prompts_to_process_data]\n",
    "         while len(data_chunks) < NUM_GPUS:\n",
    "             data_chunks.append([])\n",
    "    else:\n",
    "         actual_num_workers = NUM_GPUS \n",
    "         data_chunks_np = np.array_split(prompts_to_process_data, actual_num_workers)\n",
    "         data_chunks = [chunk.tolist() for chunk in data_chunks_np]\n",
    "\n",
    "\n",
    "    with open(fail_log_file, 'a') as f_fail:\n",
    "        f_fail.write(\"\\n--- Phase 3 Failures (Worker Processing) ---\\n\")\n",
    "\n",
    "    pool_args = [\n",
    "        (gpu_id, data_chunks[gpu_id], MODEL_ID, WORKER_BATCH_SIZE, fail_log_file)\n",
    "        for gpu_id in range(actual_num_workers) \n",
    "         if len(data_chunks[gpu_id]) > 0\n",
    "    ]\n",
    "\n",
    "    all_results_list = []\n",
    "    print(\"parallel part\")\n",
    "    with mp.Pool(processes=actual_num_workers) as pool:\n",
    "        try:\n",
    "            all_results_list = pool.starmap(inference_worker_batch, pool_args)\n",
    "        except Exception as pool_exc:\n",
    "             print(\"CRITICAL ERROR  PARALLEL PART\")\n",
    "             print(f\"error: {pool_exc}\")\n",
    "             import traceback\n",
    "             traceback.print_exc()\n",
    "\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Parallel part took: {end_time - start_time:.2f} secs.\")\n",
    "\n",
    "\n",
    "\n",
    "    all_processed_items = list(chain.from_iterable(all_results_list))\n",
    "    print(f\"Got: {len(all_processed_items)} captions\")\n",
    "\n",
    "    successful_captions = [item for item in all_processed_items if item['status'] == 'success' and item.get('generated_caption')]\n",
    "    failed_items_from_workers = [item for item in all_processed_items if item['status'] == 'failed']\n",
    "\n",
    "    newly_processed_count = len(successful_captions)\n",
    "    failed_during_processing_count = len(failed_items_from_workers)\n",
    "\n",
    "    print(f\"Successfully generated {newly_processed_count} new captions.\")\n",
    "    print(f\"Detected {failed_during_processing_count} failures during worker processing.\")\n",
    "\n",
    "    if successful_captions:\n",
    "        df_new_captions = pd.DataFrame(successful_captions)[['image_path', 'generated_caption', 'config_num']]\n",
    "\n",
    "        try:\n",
    "            file_exists = os.path.exists(caption_cache_file)\n",
    "            is_empty = not file_exists or os.path.getsize(caption_cache_file) == 0\n",
    "\n",
    "            df_new_captions.to_csv(caption_cache_file, mode='a', header=is_empty, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR sending to cache file: {e}\")\n",
    "            temp_fail_path = f\"{caption_cache_file}.failed_append_{datetime.datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
    "            df_new_captions.to_csv(temp_fail_path, index=False)\n",
    "            print(f\"failed data at: {temp_fail_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"Not working!!\")\n",
    "\n",
    "\n",
    "    print(\"FINAL PROCESSED NUMBERS: \")\n",
    "    print(f\"images processed successfully: {newly_processed_count}\")\n",
    "    total_failures_reported = skipped_invalid_count + failed_during_processing_count\n",
    "    print(f\"failed: {total_failures_reported}\")\n",
    "\n",
    "\n",
    "print(f\"\\nScript finished at: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ada9d2-2b45-434a-87d5-15354b146473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8febf2a3-1476-46dd-983d-c8e1bc8cdd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cot_prompt = \"\"\"Generate a retrieval-optimized caption through these steps:\n",
    "1. VISUAL FOUNDATION (Image Analysis Only):\n",
    "Identify primary visualization type and core components\n",
    "Catalog essential elements: axes, labels, legends, data representations\n",
    "Note quantitative relationships and spatial patterns\n",
    "Flag ambiguous elements requiring contextual clarification\n",
    "2. CONTEXTUAL MEANING (Text Analysis Only):\n",
    "Extract research purpose and experimental focus\n",
    "Identify domain-specific terminology and claims\n",
    "Note methodological parameters and key hypotheses\n",
    "Highlight contextual expectations for the figure\n",
    "3. ALIGNED SYNTHESIS (Intelligent Combination):\n",
    "Visual-Text Alignment: Where context describes visuals, incorporate directly\n",
    "Visual-Only Elements: Describe objectively without forcing textual justification\n",
    "Context-Only Insights: Include critical claims without visual evidence \n",
    "Conflict Resolution: Note contradictions between text and visuals\n",
    "Formulate integrated overview of purpose + main subject\n",
    "4. CAPTION GENERATION (Composition Guidelines):\n",
    "Compose a single 300-400 word paragraph that:\n",
    "â€¢ Starts with figure purpose and main subject\n",
    "â€¢ Integrates visual components with contextual meaning\n",
    "â€¢ Explains relationships using domain terminology\n",
    "â€¢ Incorporates keywords naturally (technical + plain-language)\n",
    "â€¢ Maintains objective tone without speculation\n",
    "5. SELF-VALIDATION (Quality Control):\n",
    "Verify claims traceable to visual/textual evidence\n",
    "Confirm no unsupported speculation\n",
    "Ensure 300-400 word length\n",
    "Check keyword coverage for retrieval\n",
    "Validate objective description of visual-textual relationships\n",
    "Remove redundant statements\n",
    "Check for completeness of visual information\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad30eb1d-6519-487a-a27e-a8fe61602ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_prompt(row, config_num):   #\n",
    "    prompt_components = []\n",
    "\n",
    "    if config_num == 0:\n",
    "        prompt_components.append(f\"[CAPTION-START]{row['original_caption']}[CAPTION-END]\\n\")\n",
    "    elif config_num == 1:\n",
    "        prompt_components.append(f\"[TITLE-START]{ row['title']}[TITLE-END]\\n\")\n",
    "        prompt_components.append(f\"[ABSTRACT-START]{row['abstract']}[ABSTRACT-END]\\n\")\n",
    "        prompt_components.append(f\"[KEYWORDS-START]{row['keywords']}[KEYWORDS-END]\\n\")\n",
    "    elif config_num == 2:\n",
    "        sentences = row.get('sentences')\n",
    "        if hasattr(sentences, '__len__') and len(sentences) > 0:\n",
    "             if isinstance(sentences, (list, np.ndarray)):\n",
    "                  sentences_str = \" \".join(map(str, sentences)) \n",
    "             else:\n",
    "                  sentences_str = str(sentences)\n",
    "             prompt_components.append(f\"[MENTION-START]{sentences_str}[MENTION-END]\\n\")\n",
    "            \n",
    "        prompt_components.append(f\"[SECTION-START] This Figure is from the section: {row['mention_section']}[SECTION-END]\\n\")\n",
    "    elif config_num == 3:\n",
    "        sentences = row.get('sentences')\n",
    "        if hasattr(sentences, '__len__') and len(sentences) > 0:\n",
    "             if isinstance(sentences, (list, np.ndarray)):\n",
    "                  sentences_str = \" \".join(map(str, sentences)) \n",
    "             else:\n",
    "                  sentences_str = str(sentences)\n",
    "             prompt_components.append(f\"[MENTION-START]{sentences_str}[MENTION-END]\\n\")\n",
    "            \n",
    "        prompt_components.append(f\"[KEYWORDS-START]{row['keywords']}[KEYWORDS-END]\\n\")\n",
    "        prompt_components.append(f\"[CAPTION-START]{row['original_caption']}[CAPTION-END]\\n\")\n",
    "            \n",
    "    else:\n",
    "        print('INCORRECT CONFIG_NUM')\n",
    "\n",
    "    context_str = \"\".join(prompt_components)\n",
    "    final_prompt_instr = base_repo_prompt  \n",
    "\n",
    "    return final_prompt_instr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d2a08c-dd11-4c74-9658-f2865c07536a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if num_prompts_to_process == 0:\n",
    "    print(\"no new images.\")\n",
    "else:\n",
    "    print(f\"using {NUM_GPUS} GPUs\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if num_prompts_to_process < NUM_GPUS:\n",
    "         actual_num_workers = num_prompts_to_process\n",
    "         print(\"Warning: less items than GPUs\")\n",
    "         data_chunks = [[item] for item in prompts_to_process_data]\n",
    "         while len(data_chunks) < NUM_GPUS:\n",
    "             data_chunks.append([])\n",
    "    else:\n",
    "         actual_num_workers = NUM_GPUS \n",
    "         data_chunks_np = np.array_split(prompts_to_process_data, actual_num_workers)\n",
    "         data_chunks = [chunk.tolist() for chunk in data_chunks_np]\n",
    "\n",
    "\n",
    "    with open(fail_log_file, 'a') as f_fail:\n",
    "        f_fail.write(\"\\n--- Phase 3 Failures (Worker Processing) ---\\n\")\n",
    "\n",
    "    pool_args = [\n",
    "        (gpu_id, data_chunks[gpu_id], MODEL_ID, WORKER_BATCH_SIZE, fail_log_file)\n",
    "        for gpu_id in range(actual_num_workers) \n",
    "         if len(data_chunks[gpu_id]) > 0\n",
    "    ]\n",
    "\n",
    "    all_results_list = []\n",
    "    print(\"parallel part\")\n",
    "    if actual_num_workers == 1:\n",
    "        gpu_id, data_chunk, model_id, worker_batch_size, fail_log_file = pool_args[0]\n",
    "        result = inference_worker_batch(gpu_id, data_chunk, model_id, worker_batch_size, fail_log_file)\n",
    "        all_results_list = [result]\n",
    "    else:\n",
    "        with mp.Pool(processes=actual_num_workers) as pool:\n",
    "            try:\n",
    "                all_results_list = pool.starmap(inference_worker_batch, pool_args)\n",
    "            except Exception as pool_exc:\n",
    "                print(\"CRITICAL ERROR  PARALLEL PART\")\n",
    "                print(f\"error: {pool_exc}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    print(f\"Parallel part took: {end_time - start_time:.2f} secs.\")\n",
    "\n",
    "    all_processed_items = list(chain.from_iterable(all_results_list))\n",
    "    print(f\"Got: {len(all_processed_items)} captions\")\n",
    "\n",
    "    successful_captions = [item for item in all_processed_items if item['status'] == 'success' and item.get('generated_caption')]\n",
    "    failed_items_from_workers = [item for item in all_processed_items if item['status'] == 'failed']\n",
    "\n",
    "    newly_processed_count = len(successful_captions)\n",
    "    failed_during_processing_count = len(failed_items_from_workers)\n",
    "\n",
    "    print(f\"Successfully generated {newly_processed_count} new captions.\")\n",
    "    print(f\"Detected {failed_during_processing_count} failures during worker processing.\")\n",
    "\n",
    "    if successful_captions:\n",
    "        df_new_captions = pd.DataFrame(successful_captions)[['image_path', 'generated_caption', 'config_num']]\n",
    "\n",
    "        try:\n",
    "            file_exists = os.path.exists(caption_cache_file)\n",
    "            is_empty = not file_exists or os.path.getsize(caption_cache_file) == 0\n",
    "\n",
    "            df_new_captions.to_csv(caption_cache_file, mode='a', header=is_empty, index=False)\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR sending to cache file: {e}\")\n",
    "            temp_fail_path = f\"{caption_cache_file}.failed_append_{datetime.datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
    "            df_new_captions.to_csv(temp_fail_path, index=False)\n",
    "            print(f\"failed data at: {temp_fail_path}\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"Not working!!\")\n",
    "\n",
    "\n",
    "    print(\"FINAL PROCESSED NUMBERS: \")\n",
    "    print(f\"images processed successfully: {newly_processed_count}\")\n",
    "    total_failures_reported = skipped_invalid_count + failed_during_processing_count\n",
    "    print(f\"failed: {total_failures_reported}\")\n",
    "\n",
    "\n",
    "print(f\"\\nScript finished at: {datetime.datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# FINAL: EVAL BY L(V)LM of generated captions + human eval on rubrics\n"
   ],
   "metadata": {},
   "id": "d50bab79-357a-47cf-a77a-48547b5f3644"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Generating prompts first\n",
    "IMAGE_BASE_PATH = \"./data/399k_imgs/\"\n",
    "\n",
    "prompts_to_process_data = [] \n",
    "skipped_cached_count = 0\n",
    "skipped_invalid_count = 0\n",
    "invalid_paths_logged_phase2 = set()\n",
    "\n",
    "with open(fail_log_file, 'w') as f_fail:\n",
    "     f_fail.write(f\"Captioning Fail Log - Run Started: {datetime.datetime.now()}\\n\")\n",
    "     f_fail.write(\"--- Phase 2 Failures (Path/Prompt Gen) ---\\n\")\n",
    "\n",
    "print(\"generating prompts\")\n",
    "for index, row in tqdm(df_input.iterrows(), total=len(df_input), desc=\"Checking Images & Generating Prompts\"):\n",
    "    row_dict = row.to_dict()\n",
    "    row_index_for_log = index\n",
    "\n",
    "    relative_image_path = row_dict.get(IMAGE_COLUMN_NAME)\n",
    "    if not relative_image_path or pd.isna(relative_image_path):\n",
    "        log_key = f\"Row {row_index_for_log}: Missing Path\"\n",
    "        if log_key not in invalid_paths_logged_phase2:\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"Skipped row {row_index_for_log}: Missing image path in DataFrame\\n\")\n",
    "            invalid_paths_logged_phase2.add(log_key)\n",
    "        skipped_invalid_count += 1\n",
    "        continue\n",
    "\n",
    "    full_image_path = os.path.join(IMAGE_BASE_PATH, str(relative_image_path))\n",
    "    full_image_path = os.path.normpath(full_image_path)\n",
    "\n",
    "    if not os.path.exists(full_image_path):\n",
    "        log_key = f\"no file: {full_image_path}\"\n",
    "        if log_key not in invalid_paths_logged_phase2:\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"File not found at '{full_image_path}'\\n\")\n",
    "            invalid_paths_logged_phase2.add(log_key)\n",
    "        skipped_invalid_count += 1\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prompt = generate_prompt(row_dict, 3)\n",
    "        prompt_data = {\n",
    "            'image_path': full_image_path,\n",
    "            'prompt': prompt,\n",
    "            'config_num': 3 \n",
    "        }\n",
    "        prompts_to_process_data.append(prompt_data)\n",
    "        with open(prompts_to_process_file, 'a') as f_prompts:\n",
    "            f_prompts.write(json.dumps(prompt_data) + '\\n')\n",
    "\n",
    "    except Exception as e:\n",
    "        log_key = f\"Prompt Gen Error: {full_image_path}\"\n",
    "        if log_key not in invalid_paths_logged_phase2:\n",
    "            print(f\"Failed to generate prompt for row {row_index_for_log}, image: {full_image_path}. Error: {e}\")\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"Failed row {row_index_for_log} during prompt generation: {full_image_path} | Error: {str(e)}\\n\")\n",
    "            invalid_paths_logged_phase2.add(log_key)\n",
    "        skipped_invalid_count += 1\n",
    "        continue\n",
    "\n",
    "num_prompts_to_process = len(prompts_to_process_data)\n",
    "print(f\"\\nIdentified {num_prompts_to_process} new images\")\n",
    "print(f\"Skipped {skipped_cached_count} in cache.\")\n",
    "print(f\"Skipped {skipped_invalid_count} rows\")\n",
    "if skipped_invalid_count > 0:\n",
    "     print(f\"(Check '{fail_log_file}' for details on failed items)\")\n",
    "if num_prompts_to_process > 0:\n",
    "    print(f\"Details of items to process saved to: {prompts_to_process_file}\")"
   ],
   "metadata": {},
   "id": "f80f8942-81d4-46b8-9897-dbe41b351a02"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc9b2f-628c-420a-b962-48f7f46e5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_eval_rubrics = \"\"\"Technical Correctness: Accuracy of scientific terminology, quantitative values, and conceptual relationships depicted in the figure.\n",
    "Scoring Scale:\n",
    "Incorrect (1): Contains factual errors or misrepresents data relationships\n",
    "Partially Correct (2): Generally accurate but contains minor technical inaccuracies \n",
    "Mostly Correct (3): Precise technical language with isolated omissions \n",
    "Fully Correct (4): Technically precise with complete quantitative details \n",
    "Completeness: Coverage of all critical visual elements and conceptual components necessary for figure interpretation.\n",
    "Scoring Scale:\n",
    "Incomplete (1): Misses >50% of salient elements \n",
    "Partially Complete (2): Covers primary elements with notable gaps \n",
    "Substantially Complete (3): Includes most elements with minor omissions \n",
    "Exhaustively Complete (4): Comprehensive coverage including secondary elements \n",
    "Conciseness & Focus: Precision in highlighting essential information while eliminating redundancy\n",
    "Scoring Scale:\n",
    "Redundant/Unfocused (1): Contains significant extraneous information obscuring key insights\n",
    "Partially Focused (2): Communicates main points but with noticeable digressions\n",
    "Mostly Concise (3): Direct presentation with minor non-essential details\n",
    "Precisely Focused (4): Economical delivery of maximum relevant information\n",
    "Method-Context Integration: Appropriate incorporation of experimental methodology relevant to figure interpretation.\n",
    "Scoring Scale:\n",
    "Absent (1): No methodological references \n",
    "Basic (2): Generic method mentions\n",
    "Contextualized (3): Specific technique references \n",
    "Interpretive (4): Methodological details enabling critical analysis \n",
    "Visual-Context Synthesis: Effective utilization of visual elements beyond basic description.\n",
    "Scoring Scale:\n",
    "Superficial (1): Basic element listing \n",
    "Descriptive (2): Visual feature identification \n",
    "Analytical (3): Integrated visual-textual analysis \n",
    "Interpretive (4): Synthesized visual-data insights\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87532949-c48f-4026-b88b-e3b0abca57a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_VERSION = \"2024-02-15-preview\"\n",
    "MODEL_ID = \"gpt4o-240513\"\n",
    "\n",
    "def get_azure_client(): \n",
    "    return AzureOpenAI(\n",
    "        azure_endpoint=AZURE_ENDPOINT,\n",
    "        api_key=AZURE_API_KEY,\n",
    "        api_version=API_VERSION\n",
    "    )\n",
    "\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode('ascii')\n",
    "\n",
    "def inference_worker_batch(worker_id, data_chunk, model_id, batch_size, fail_log_file):\n",
    "    client = get_azure_client()\n",
    "    results = []\n",
    "    \n",
    "    for item in data_chunk:\n",
    "        image_path = item['image_path']\n",
    "        try:\n",
    "            base64_image = encode_image(image_path)\n",
    "            messages = [{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": item['prompt']},\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
    "                }]\n",
    "            }]\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_ID,\n",
    "                messages=messages,\n",
    "                max_tokens=512\n",
    "            )\n",
    "            \n",
    "            results.append({\n",
    "                'image_path': image_path,\n",
    "                'generated_caption': response.choices[0].message.content.strip(),\n",
    "                'status': 'success',\n",
    "                'error': None\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            results.append({\n",
    "                'image_path': image_path,\n",
    "                'generated_caption': None,\n",
    "                'status': 'failed',\n",
    "                'error': error_msg\n",
    "            })\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"[Worker {worker_id}] Error processing {image_path}: {error_msg}\\n\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ce927c-4725-452e-bed2-af125efbdb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "'grfe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b8054-d290-4c42-a56c-19293871634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import base64\n",
    "from openai import AzureOpenAI\n",
    "import time\n",
    "\n",
    "evaluation_prompt = \"\"\"Evaluate the provided figure caption based on the image, context, and these criteria:\n",
    "\n",
    "[FIGURE CONTEXT]\n",
    "{context_str}\n",
    "\n",
    "[GENERATED CAPTION]\n",
    "{generated_caption}\n",
    "\n",
    "[EVALUATION RUBRICS]\n",
    "{base_eval_rubrics}\n",
    "\n",
    "[INSTRUCTIONS]\n",
    "1. Carefully examine the figure and context\n",
    "2. For each rubric category:\n",
    "   - Compare caption content against visual evidence\n",
    "   - Check alignment with contextual information\n",
    "   - Assign score (1-4) based on rubric definitions\n",
    "   - Provide 1-sentence justification referencing specific evidence\n",
    "3. Maintain strict objectivity: Base scores only on visible/contextual evidence\n",
    "4. Output JSON format: {{\"rubric\": {{\"score\": int, \"justification\": str}}}}\n",
    "\n",
    "[OUTPUT REQUIREMENTS]\n",
    "- Valid JSON object with all 5 rubrics\n",
    "- Scores must be integers 1-4\n",
    "- Justifications reference figure/context specifics\n",
    "- No additional commentary\n",
    "\"\"\"\n",
    "\n",
    "def build_context_str(context_row):\n",
    "    prompt_components = []\n",
    "    sentences = context_row.get('sentences')\n",
    "    if hasattr(sentences, '__len__') and len(sentences) > 0:\n",
    "        if isinstance(sentences, (list, np.ndarray)):\n",
    "            sentences_str = \" \".join(map(str, sentences))\n",
    "        else:\n",
    "            sentences_str = str(sentences)\n",
    "        prompt_components.append(f\"[MENTION-START]{sentences_str}[MENTION-END]\\n\")\n",
    "    prompt_components.append(f\"[KEYWORDS-START]{context_row['keywords']}[KEYWORDS-END]\\n\")\n",
    "    prompt_components.append(f\"[CAPTION-START]{context_row['original_caption']}[CAPTION-END]\\n\")\n",
    "    return \"\".join(prompt_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ced7c83-cb98-43e1-a8a7-b1b9c160d64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_captions_with_vllm(df, context_df, fail_log_file=\"fail_log.txt\"):\n",
    "    client = get_azure_client()\n",
    "    eval_answers = []\n",
    "\n",
    "    context_lookup = context_df.set_index('image_path').to_dict('index')\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        og_path = row['image_path']\n",
    "        image_path = og_path.split('/')[-1]\n",
    "        generated_caption = row['generated_caption']\n",
    "        context_row = context_lookup.get(image_path)\n",
    "        if context_row is None:\n",
    "            eval_answers.append({'status': 'failed', 'error': 'Context not found'})\n",
    "            continue\n",
    "\n",
    "        context_str = build_context_str(context_row)\n",
    "\n",
    "        prompt = evaluation_prompt.format(\n",
    "            context_str=context_str,\n",
    "            generated_caption=generated_caption,\n",
    "            base_eval_rubrics=base_eval_rubrics\n",
    "        )\n",
    "\n",
    "        #print('here:  ',  prompt, image_path)\n",
    "        try:\n",
    "            base64_image = encode_image(og_path)\n",
    "        except Exception as e:\n",
    "            eval_answers.append({'status': 'failed', 'error': f'Image encoding error: {str(e)}'})\n",
    "            continue\n",
    "\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "        print(time.time())\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_ID,\n",
    "                messages=messages,\n",
    "                max_tokens=512\n",
    "            )\n",
    "            eval_answer = response.choices[0].message.content.strip()\n",
    "            eval_answers.append(eval_answer)\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            eval_answers.append({'status': 'failed', 'error': error_msg})\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"Error processing {image_path}: {error_msg}\\n\")\n",
    "\n",
    "        time.sleep(8)\n",
    "\n",
    "\n",
    "    df['eval_answer'] = eval_answers\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd42d13a-824e-4086-81b6-9ed1269735e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def try_4o_cot(df, context_df, fail_log_file=\"fail_log.txt\"):\n",
    "    client = get_azure_client()\n",
    "    eval_answers = []\n",
    "\n",
    "    context_lookup = context_df.set_index('image_path').to_dict('index')\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        og_path = row['image_path']\n",
    "        image_path = 'data/399k_imgs/' + og_path\n",
    "        context_row = context_lookup.get(og_path)\n",
    "        if context_row is None:\n",
    "            eval_answers.append({'status': 'failed', 'error': 'Context not found'})\n",
    "            continue\n",
    "\n",
    "        context_str = build_context_str(context_row)\n",
    "        prompt = context_str + \"\\n\" +  base_cot_prompt\n",
    "\n",
    "        #print('here:  ',  prompt, image_path)\n",
    "        try:\n",
    "            base64_image = encode_image(image_path)\n",
    "        except Exception as e:\n",
    "            eval_answers.append({'status': 'failed', 'error': f'Image encoding error: {str(e)}'})\n",
    "            continue\n",
    "\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "        print(time.time())\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_ID,\n",
    "                messages=messages,\n",
    "                max_tokens=512\n",
    "            )\n",
    "            answer = response.choices[0].message.content.strip()\n",
    "            eval_answers.append(answer)\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            eval_answers.append({'status': 'failed', 'error': error_msg})\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"Error processing {image_path}: {error_msg}\\n\")\n",
    "\n",
    "        time.sleep(3)\n",
    "\n",
    "\n",
    "    df['generated_caption'] = eval_answers\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cdc785-54a2-4cb5-9da2-b70231c74dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = evaluate_captions_with_vllm(\n",
    "    df=df_1k.iloc[1698:],   #[1436:], #   + 262\n",
    "    context_df=df_full_texts,\n",
    "    cache_file_path=\"cache/caption_exp_contexts_4.csv\",\n",
    "    experiment_name=\"context\",\n",
    "    fail_log_file=\"cache/errors_exp.log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d17ef6-c444-41e9-b42d-8a0c01f222a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_2 = evaluate_captions_with_vllm(\n",
    "    df=new_cot.iloc[100:],\n",
    "    context_df=df_full_texts,\n",
    "    cache_file_path=\"cache/caption_exp_cpm_cot_2.csv\",\n",
    "    experiment_name=\"cot\",\n",
    "    fail_log_file=\"cache/errors_exp.log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4bf65-64da-46a8-9568-0c17b4397fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_3 = evaluate_captions_with_vllm(\n",
    "    df=filtered_4o.iloc[100:],\n",
    "    context_df=df_full_texts,\n",
    "    cache_file_path=\"cache/caption_exp_4o_2.csv\",\n",
    "    experiment_name=\"4o\",\n",
    "    fail_log_file=\"cache/errors_exp.log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a6cfb5-65f7-4659-8787-16710dc03483",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_4 = evaluate_captions_with_vllm(\n",
    "    df=filtered_cpm.iloc[225:],\n",
    "    context_df=df_full_texts,\n",
    "    cache_file_path=\"cache/caption_exp_cpm_2.csv\",\n",
    "    experiment_name=\"cpm\",\n",
    "    fail_log_file=\"cache/errors_exp.log\"\n",
    ")\n",
    "\n",
    "results_5 = evaluate_captions_with_vllm(\n",
    "    df=filtered_qwen.iloc[100:],\n",
    "    context_df=df_full_texts,\n",
    "    cache_file_path=\"cache/caption_exp_qwen_2.csv\",\n",
    "    experiment_name=\"qwen\",\n",
    "    fail_log_file=\"cache/errors_exp.log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154cddcb-3587-4879-9273-5e554acb5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_captions_with_vllm(df, context_df, cache_file_path, experiment_name, fail_log_file=\"fail_log.txt\"):\n",
    "    client = get_azure_client()\n",
    "    eval_answers = []\n",
    "    cache_dict = {}\n",
    "    \n",
    "    if os.path.exists(cache_file_path):\n",
    "        try:\n",
    "            cache_df = pd.read_csv(cache_file_path)\n",
    "\n",
    "            \n",
    "            if not cache_df.empty:  \n",
    "                for _, row in cache_df.iterrows():\n",
    "                    key = (row['image_path'], row['generated_caption'], row['experiment'])\n",
    "                    cache_dict[key] = row['eval_answer']\n",
    "        except (pd.errors.EmptyDataError, KeyError):\n",
    "            pass\n",
    "\n",
    "    os.makedirs(os.path.dirname(cache_file_path), exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(cache_file_path):\n",
    "        with open(cache_file_path, 'w', newline='', encoding='utf-8') as f_cache:\n",
    "            writer = csv.writer(f_cache)\n",
    "            writer.writerow(['image_path', 'generated_caption', 'experiment', 'eval_answer'])\n",
    "\n",
    "    context_lookup = context_df.set_index('image_path').to_dict('index')\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        image_path = row['image_path']\n",
    "        og_path = 'data/399k_imgs/' + image_path   \n",
    "        generated_caption = row['generated_caption']\n",
    "        if type(generated_caption) != str:\n",
    "            continue\n",
    "        cache_key = (og_path, generated_caption, experiment_name)\n",
    "\n",
    "        if cache_key in cache_dict:\n",
    "            eval_answers.append(cache_dict[cache_key])\n",
    "            continue\n",
    "\n",
    "        context_row = context_lookup.get(image_path)\n",
    "        if context_row is None:\n",
    "            error_str = json.dumps({'status': 'failed', 'error': 'Context not found'})\n",
    "            eval_answers.append(error_str)\n",
    "            cache_dict[cache_key] = error_str\n",
    "            continue\n",
    "\n",
    "        context_str = build_context_str(context_row)\n",
    "        prompt = evaluation_prompt.format(\n",
    "            context_str=context_str,\n",
    "            generated_caption=generated_caption,\n",
    "            base_eval_rubrics=base_eval_rubrics\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            base64_image = encode_image(og_path)\n",
    "        except Exception as e:\n",
    "            error_str = json.dumps({'status': 'failed', 'error': f'Image encoding error: {str(e)}'})\n",
    "            eval_answers.append(error_str)\n",
    "            cache_dict[cache_key] = error_str\n",
    "            continue\n",
    "\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_ID,\n",
    "                messages=messages,\n",
    "                max_tokens=512\n",
    "            )\n",
    "            eval_answer = response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            eval_answer = json.dumps({'status': 'failed', 'error': str(e)})\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"Error processing {og_path}: {str(e)}\\n\")\n",
    "        \n",
    "        with open(cache_file_path, 'a', newline='', encoding='utf-8') as f_cache:\n",
    "            writer = csv.writer(f_cache)\n",
    "            writer.writerow([og_path, generated_caption, experiment_name, eval_answer])\n",
    "        \n",
    "        cache_dict[cache_key] = eval_answer\n",
    "        eval_answers.append(eval_answer)\n",
    "        time.sleep(3)\n",
    "\n",
    "    df['eval_answer'] = eval_answers\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd811bd-1f60-4e01-a8aa-12abcd13369f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#same function but using the original captions instead of the generated ones for evaluation\n",
    "def evaluate_captions_with_vllm(df, context_df, cache_file_path, experiment_name, fail_log_file=\"fail_log.txt\"):\n",
    "    client = get_azure_client()\n",
    "    eval_answers = []\n",
    "    cache_dict = {}\n",
    "    \n",
    "    if os.path.exists(cache_file_path):\n",
    "        try:\n",
    "            cache_df = pd.read_csv(cache_file_path)\n",
    "\n",
    "            \n",
    "            if not cache_df.empty:  \n",
    "                for _, row in cache_df.iterrows():\n",
    "                    key = (row['image_path'], row['generated_caption'], row['experiment'])\n",
    "                    cache_dict[key] = row['eval_answer']\n",
    "        except (pd.errors.EmptyDataError, KeyError):\n",
    "            pass\n",
    "\n",
    "    os.makedirs(os.path.dirname(cache_file_path), exist_ok=True)\n",
    "    \n",
    "    if not os.path.exists(cache_file_path):\n",
    "        with open(cache_file_path, 'w', newline='', encoding='utf-8') as f_cache:\n",
    "            writer = csv.writer(f_cache)\n",
    "            writer.writerow(['image_path', 'generated_caption', 'experiment', 'eval_answer'])\n",
    "\n",
    "    context_lookup = context_df.set_index('image_path').to_dict('index')\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        image_path = row['image_path']\n",
    "        small_path = image_path.split('/')[-1]\n",
    "        og_path = image_path\n",
    "        generated_caption = context_lookup.get(small_path).get('original_caption') \n",
    "        if type(generated_caption) != str:\n",
    "            continue\n",
    "        cache_key = (og_path, generated_caption, experiment_name)\n",
    "\n",
    "        if cache_key in cache_dict:\n",
    "            eval_answers.append(cache_dict[cache_key])\n",
    "            continue\n",
    "\n",
    "        context_row = context_lookup.get(small_path)\n",
    "        if context_row is None:\n",
    "            error_str = json.dumps({'status': 'failed', 'error': 'Context not found'})\n",
    "            eval_answers.append(error_str)\n",
    "            cache_dict[cache_key] = error_str\n",
    "            continue\n",
    "\n",
    "        context_str = build_context_str(context_row)\n",
    "        prompt = evaluation_prompt.format(\n",
    "            context_str=context_str,\n",
    "            generated_caption=generated_caption,\n",
    "            base_eval_rubrics=base_eval_rubrics\n",
    "        )\n",
    "        #print('THIS PROMPT IS:  ',prompt )\n",
    "        try:\n",
    "            base64_image = encode_image(og_path)\n",
    "        except Exception as e:\n",
    "            error_str = json.dumps({'status': 'failed', 'error': f'Image encoding error: {str(e)}'})\n",
    "            eval_answers.append(error_str)\n",
    "            cache_dict[cache_key] = error_str\n",
    "            continue\n",
    "\n",
    "        messages = [{\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": prompt},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}}\n",
    "            ]\n",
    "        }]\n",
    "\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=MODEL_ID,\n",
    "                messages=messages,\n",
    "                max_tokens=512\n",
    "            )\n",
    "            eval_answer = response.choices[0].message.content.strip()\n",
    "        except Exception as e:\n",
    "            eval_answer = json.dumps({'status': 'failed', 'error': str(e)})\n",
    "            with open(fail_log_file, 'a') as f_fail:\n",
    "                f_fail.write(f\"Error processing {og_path}: {str(e)}\\n\")\n",
    "        \n",
    "        with open(cache_file_path, 'a', newline='', encoding='utf-8') as f_cache:\n",
    "            writer = csv.writer(f_cache)\n",
    "            writer.writerow([og_path, generated_caption, experiment_name, eval_answer])\n",
    "        \n",
    "        cache_dict[cache_key] = eval_answer\n",
    "        eval_answers.append(eval_answer)\n",
    "        time.sleep(3)\n",
    "\n",
    "    df['eval_answer'] = eval_answers\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82180c60-cd8f-4530-a083-8f33c61c6f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to pick images for (human) evaluation:\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def categorize_images(df):\n",
    "    result_df = pd.DataFrame(columns=['image_path', 'category'])\n",
    "    \n",
    "    os.makedirs('temp_images', exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        working_df = df.sample(frac=1)\n",
    "        for idx, row in df.iterrows():\n",
    "            img_path = row['image_path']\n",
    "            \n",
    "            if not os.path.exists(img_path):\n",
    "                print(f\"Image not found: {img_path}\")\n",
    "                continue\n",
    "                \n",
    "            img = Image.open(img_path)\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            plt.title(f\"Image {idx+1}/{len(df)}\")\n",
    "            plt.show()\n",
    "            while True:\n",
    "                action = input(\"Action? [y=include, n=skip, s=stop]: \").strip().lower()\n",
    "                \n",
    "                if action == 's':  \n",
    "                    print(\"Stopping categorization...\")\n",
    "                    return result_df\n",
    "                    \n",
    "                elif action == 'n':  \n",
    "                    print(\"Skipping image...\")\n",
    "                    plt.close()\n",
    "                    break\n",
    "                    \n",
    "                elif action == 'y':  \n",
    "                    category = input(\"Enter category for this image: \").strip()\n",
    "                    if not category:\n",
    "                        print(\"Category cannot be empty!\")\n",
    "                        continue\n",
    "                        \n",
    "                    result_df = pd.concat([\n",
    "                        result_df,\n",
    "                        pd.DataFrame([{'image_path': img_path, 'category': category}])\n",
    "                    ], ignore_index=True)\n",
    "                    \n",
    "                    print(f\"Added as: {category}\")\n",
    "                    plt.close()\n",
    "                    break\n",
    "                    \n",
    "                else:\n",
    "                    print(\"Invalid input! Please enter y, n, or s\")\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nProcess interrupted by user\")\n",
    "    \n",
    "    plt.close('all')  \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfff0ba0-1954-4e28-81a1-8ff46f8b3e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dc0839-3000-4695-a9bc-7bf858ac2e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN, CPM_COT, 4o, 4oCoT, 1= original caption only, 2= title, abstract, keywords; 3: sentences, mention_section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c13aab5-f030-4aaa-a916-530588343058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#for human evaluation\n",
    "display_image_and_captions('data/399k_imgs/S0098135407000828_fig_gr2.jpg', df4, df_mini_cot, df_4o, df_4o_cot, df1, df2, df3, context_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "AZURE_API_KEY=\"\"\n",
    "AZURE_ENDPOINT=\"\""
   ],
   "metadata": {},
   "id": "39854fcc-b125-4d9d-8926-5b02dfd2737e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7c877b-8511-49b1-801c-c26187fe2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "def display_image_and_captions(image_path, df1, df2, df3, df4, df5, df6, df7, context_df):\n",
    "    full_image_path = image_path  #f'data/images/{image_path}'\n",
    "    try:\n",
    "        img = mpimg.imread(full_image_path)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f'Image: {image_path}')\n",
    "        plt.show()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Image not found: {full_image_path}\")\n",
    "        return\n",
    "    \n",
    "    caption_dfs = [df1, df2, df3, df4, df5, df6, df7]\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Generated Captions from Six Models:\")\n",
    "    print(\"=\"*50)\n",
    "    for i, df in enumerate(caption_dfs, 1):\n",
    "        matches = df[df['image_path'] == image_path]\n",
    "        \n",
    "        if matches.empty:\n",
    "            print(f\"Model {i}: No caption found for '{image_path}'\\n\")\n",
    "        else:\n",
    "            caption = matches.iloc[0]['generated_caption']\n",
    "            print(f\"Model {i} Caption:\\n{caption}\\n{'-'*50}\\n\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Contextual Information:\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    context_matches = context_df[context_df['image_path'] == image_path.split('/')[-1]]\n",
    "    \n",
    "    if context_matches.empty:\n",
    "        print(f\"No contextual information found for '{image_path}'\")\n",
    "    else:\n",
    "        context_data = context_matches.iloc[0] \n",
    "        print(f\"Sentences:\\n{context_data['sentences']}\\n\")\n",
    "        print(f\"Keywords:\\n{context_data['keywords']}\\n\")\n",
    "        print(f\"Original Caption:\\n{context_data['original_caption']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
